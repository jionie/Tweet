
Loading data...
*General Setting*
seed: 42
model: TweetBert
trainable parameters:127,011,849
model's state_dict:
device: cuda
use gpu: True
device num: 2
optimizer: AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-08
    initial_lr: 1e-05
    lr: 0.0
    weight_decay: 0.001

Parameter Group 1
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-08
    initial_lr: 0.0002
    lr: 0.0
    weight_decay: 0.001

Parameter Group 2
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-08
    initial_lr: 1e-05
    lr: 0.0
    weight_decay: 0.0

Parameter Group 3
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-08
    initial_lr: 0.0002
    lr: 0.0
    weight_decay: 0.0
)
reuse model: False
adversarial training: False
** start training here! **
   batch_size=16,  accumulation_steps=1
   experiment  = ['training-k-fold-bert-v2.py']
Epoch0

lr: 0.000071 train loss: 2.521106 train_jaccard: 0.538121 train_jaccard_postprocessing: 0.974374 train_jaccard_no_postprocessing: 0.219625
lr: 0.000104 train loss: 1.325165 train_jaccard: 0.572832 train_jaccard_postprocessing: 0.971973 train_jaccard_no_postprocessing: 0.278145
lr: 0.000103 train loss: 1.132620 train_jaccard: 0.594327 train_jaccard_postprocessing: 0.972433 train_jaccard_no_postprocessing: 0.319143
validation loss: 0.785965 eval_jaccard: 0.677642 eval_jaccard_postprocessing: 0.972674 eval_jaccard_no_postprocessing: 0.469039
Validation metric improved (-inf --> 0.677642).  Saving model ...Model saved as /media/jionie/my_disk/Kaggle/Tweet/model/TweetBert/roberta-base-42/fold_3/458_step_0_epoch.pth.lr: 0.000102 train loss: 1.006448 train_jaccard: 0.613620 train_jaccard_postprocessing: 0.971306 train_jaccard_no_postprocessing: 0.346816
lr: 0.000100 train loss: 0.933115 train_jaccard: 0.624480 train_jaccard_postprocessing: 0.971988 train_jaccard_no_postprocessing: 0.373702
lr: 0.000099 train loss: 0.878432 train_jaccard: 0.634061 train_jaccard_postprocessing: 0.971947 train_jaccard_no_postprocessing: 0.388837
validation loss: 0.651547 eval_jaccard: 0.698259 eval_jaccard_postprocessing: 0.972674 eval_jaccard_no_postprocessing: 0.504233
Validation metric improved (0.677642 --> 0.698259).  Saving model ...Model saved as /media/jionie/my_disk/Kaggle/Tweet/model/TweetBert/roberta-base-42/fold_3/916_step_0_epoch.pth.lr: 0.000098 train loss: 0.862864 train_jaccard: 0.640804 train_jaccard_postprocessing: 0.971431 train_jaccard_no_postprocessing: 0.402247
lr: 0.000096 train loss: 0.872883 train_jaccard: 0.645060 train_jaccard_postprocessing: 0.970534 train_jaccard_no_postprocessing: 0.411138
lr: 0.000095 train loss: 0.862327 train_jaccard: 0.650649 train_jaccard_postprocessing: 0.970406 train_jaccard_no_postprocessing: 0.422155
lr: 0.000094 train loss: 0.855641 train_jaccard: 0.654132 train_jaccard_postprocessing: 0.970470 train_jaccard_no_postprocessing: 0.428233
validation loss: 0.623904 eval_jaccard: 0.704151 eval_jaccard_postprocessing: 0.972674 eval_jaccard_no_postprocessing: 0.514292
Validation metric improved (0.698259 --> 0.704151).  Saving model ...Model saved as /media/jionie/my_disk/Kaggle/Tweet/model/TweetBert/roberta-base-42/fold_3/1374_step_0_epoch.pth.Epoch1

lr: 0.000092 train loss: 0.762327 train_jaccard: 0.708575 train_jaccard_postprocessing: 0.971547 train_jaccard_no_postprocessing: 0.522260
lr: 0.000091 train loss: 0.801096 train_jaccard: 0.704340 train_jaccard_postprocessing: 0.971114 train_jaccard_no_postprocessing: 0.519031
lr: 0.000090 train loss: 0.765733 train_jaccard: 0.708227 train_jaccard_postprocessing: 0.972298 train_jaccard_no_postprocessing: 0.519132
validation loss: 0.630368 eval_jaccard: 0.705443 eval_jaccard_postprocessing: 0.972674 eval_jaccard_no_postprocessing: 0.516497
Validation metric improved (0.704151 --> 0.705443).  Saving model ...Model saved as /media/jionie/my_disk/Kaggle/Tweet/model/TweetBert/roberta-base-42/fold_3/1832_step_1_epoch.pth.lr: 0.000088 train loss: 0.789582 train_jaccard: 0.708550 train_jaccard_postprocessing: 0.970476 train_jaccard_no_postprocessing: 0.523325
lr: 0.000087 train loss: 0.784224 train_jaccard: 0.705778 train_jaccard_postprocessing: 0.970148 train_jaccard_no_postprocessing: 0.518965
lr: 0.000086 train loss: 0.786696 train_jaccard: 0.706359 train_jaccard_postprocessing: 0.970163 train_jaccard_no_postprocessing: 0.517100
validation loss: 0.606798 eval_jaccard: 0.708944 eval_jaccard_postprocessing: 0.972674 eval_jaccard_no_postprocessing: 0.522473
Validation metric improved (0.705443 --> 0.708944).  Saving model ...Model saved as /media/jionie/my_disk/Kaggle/Tweet/model/TweetBert/roberta-base-42/fold_3/2290_step_1_epoch.pth.lr: 0.000084 train loss: 0.781036 train_jaccard: 0.706246 train_jaccard_postprocessing: 0.971201 train_jaccard_no_postprocessing: 0.516196
lr: 0.000083 train loss: 0.782947 train_jaccard: 0.705202 train_jaccard_postprocessing: 0.971016 train_jaccard_no_postprocessing: 0.515943
lr: 0.000082 train loss: 0.795940 train_jaccard: 0.704695 train_jaccard_postprocessing: 0.971077 train_jaccard_no_postprocessing: 0.514858
lr: 0.000080 train loss: 0.776572 train_jaccard: 0.704536 train_jaccard_postprocessing: 0.970354 train_jaccard_no_postprocessing: 0.514821
validation loss: 0.611481 eval_jaccard: 0.707559 eval_jaccard_postprocessing: 0.972674 eval_jaccard_no_postprocessing: 0.520108
Epoch2

lr: 0.000079 train loss: 0.716104 train_jaccard: 0.722271 train_jaccard_postprocessing: 0.975698 train_jaccard_no_postprocessing: 0.542719
lr: 0.000078 train loss: 0.741311 train_jaccard: 0.716943 train_jaccard_postprocessing: 0.972451 train_jaccard_no_postprocessing: 0.536596
lr: 0.000076 train loss: 0.750774 train_jaccard: 0.714295 train_jaccard_postprocessing: 0.971546 train_jaccard_no_postprocessing: 0.532376
validation loss: 0.614995 eval_jaccard: 0.712014 eval_jaccard_postprocessing: 0.972674 eval_jaccard_no_postprocessing: 0.527713
Validation metric improved (0.708944 --> 0.712014).  Saving model ...Model saved as /media/jionie/my_disk/Kaggle/Tweet/model/TweetBert/roberta-base-42/fold_3/3206_step_2_epoch.pth.lr: 0.000075 train loss: 0.722332 train_jaccard: 0.718706 train_jaccard_postprocessing: 0.971159 train_jaccard_no_postprocessing: 0.541267
lr: 0.000074 train loss: 0.737922 train_jaccard: 0.717839 train_jaccard_postprocessing: 0.970117 train_jaccard_no_postprocessing: 0.539235
lr: 0.000072 train loss: 0.736357 train_jaccard: 0.718562 train_jaccard_postprocessing: 0.970042 train_jaccard_no_postprocessing: 0.540558
validation loss: 0.600138 eval_jaccard: 0.711208 eval_jaccard_postprocessing: 0.972674 eval_jaccard_no_postprocessing: 0.526338
lr: 0.000071 train loss: 0.738484 train_jaccard: 0.718334 train_jaccard_postprocessing: 0.969474 train_jaccard_no_postprocessing: 0.541404
lr: 0.000070 train loss: 0.676457 train_jaccard: 0.719954 train_jaccard_postprocessing: 0.969406 train_jaccard_no_postprocessing: 0.543177
lr: 0.000068 train loss: 0.695058 train_jaccard: 0.720855 train_jaccard_postprocessing: 0.970124 train_jaccard_no_postprocessing: 0.543139
lr: 0.000067 train loss: 0.733485 train_jaccard: 0.719729 train_jaccard_postprocessing: 0.970519 train_jaccard_no_postprocessing: 0.540874
validation loss: 0.601269 eval_jaccard: 0.712605 eval_jaccard_postprocessing: 0.972674 eval_jaccard_no_postprocessing: 0.528722
Validation metric improved (0.712014 --> 0.712605).  Saving model ...Model saved as /media/jionie/my_disk/Kaggle/Tweet/model/TweetBert/roberta-base-42/fold_3/4122_step_2_epoch.pth.Epoch3

lr: 0.000066 train loss: 0.665003 train_jaccard: 0.739375 train_jaccard_postprocessing: 0.970640 train_jaccard_no_postprocessing: 0.568312
lr: 0.000064 train loss: 0.686489 train_jaccard: 0.735379 train_jaccard_postprocessing: 0.971003 train_jaccard_no_postprocessing: 0.566865
lr: 0.000063 train loss: 0.642383 train_jaccard: 0.740612 train_jaccard_postprocessing: 0.970222 train_jaccard_no_postprocessing: 0.574023
validation loss: 0.613789 eval_jaccard: 0.709033 eval_jaccard_postprocessing: 0.972674 eval_jaccard_no_postprocessing: 0.522625
lr: 0.000062 train loss: 0.664954 train_jaccard: 0.739911 train_jaccard_postprocessing: 0.969894 train_jaccard_no_postprocessing: 0.572478
lr: 0.000060 train loss: 0.701917 train_jaccard: 0.737449 train_jaccard_postprocessing: 0.970054 train_jaccard_no_postprocessing: 0.568897
lr: 0.000059 train loss: 0.674569 train_jaccard: 0.737859 train_jaccard_postprocessing: 0.970319 train_jaccard_no_postprocessing: 0.569570
validation loss: 0.614406 eval_jaccard: 0.710827 eval_jaccard_postprocessing: 0.972674 eval_jaccard_no_postprocessing: 0.525688
lr: 0.000058 train loss: 0.679898 train_jaccard: 0.737936 train_jaccard_postprocessing: 0.970967 train_jaccard_no_postprocessing: 0.569528
lr: 0.000056 train loss: 0.667151 train_jaccard: 0.737901 train_jaccard_postprocessing: 0.971306 train_jaccard_no_postprocessing: 0.569560
lr: 0.000055 train loss: 0.715373 train_jaccard: 0.736112 train_jaccard_postprocessing: 0.970737 train_jaccard_no_postprocessing: 0.567998
lr: 0.000054 train loss: 0.680921 train_jaccard: 0.735729 train_jaccard_postprocessing: 0.970372 train_jaccard_no_postprocessing: 0.568232
validation loss: 0.616772 eval_jaccard: 0.710821 eval_jaccard_postprocessing: 0.972674 eval_jaccard_no_postprocessing: 0.525677
Epoch4

lr: 0.000052 train loss: 0.629010 train_jaccard: 0.748363 train_jaccard_postprocessing: 0.964401 train_jaccard_no_postprocessing: 0.590344
lr: 0.000051 train loss: 0.643623 train_jaccard: 0.751934 train_jaccard_postprocessing: 0.967904 train_jaccard_no_postprocessing: 0.595437
lr: 0.000049 train loss: 0.632519 train_jaccard: 0.755019 train_jaccard_postprocessing: 0.968924 train_jaccard_no_postprocessing: 0.599630
validation loss: 0.624307 eval_jaccard: 0.712578 eval_jaccard_postprocessing: 0.972674 eval_jaccard_no_postprocessing: 0.528676
lr: 0.000048 train loss: 0.654765 train_jaccard: 0.750820 train_jaccard_postprocessing: 0.969296 train_jaccard_no_postprocessing: 0.593836
lr: 0.000047 train loss: 0.655681 train_jaccard: 0.749366 train_jaccard_postprocessing: 0.969603 train_jaccard_no_postprocessing: 0.594847
lr: 0.000045 train loss: 0.625696 train_jaccard: 0.750249 train_jaccard_postprocessing: 0.970473 train_jaccard_no_postprocessing: 0.594025
validation loss: 0.617115 eval_jaccard: 0.711661 eval_jaccard_postprocessing: 0.972674 eval_jaccard_no_postprocessing: 0.527112
lr: 0.000044 train loss: 0.627895 train_jaccard: 0.749413 train_jaccard_postprocessing: 0.969758 train_jaccard_no_postprocessing: 0.591868
lr: 0.000043 train loss: 0.620684 train_jaccard: 0.750491 train_jaccard_postprocessing: 0.970359 train_jaccard_no_postprocessing: 0.592135
lr: 0.000041 train loss: 0.637872 train_jaccard: 0.750244 train_jaccard_postprocessing: 0.970494 train_jaccard_no_postprocessing: 0.592562
lr: 0.000040 train loss: 0.634023 train_jaccard: 0.749943 train_jaccard_postprocessing: 0.970474 train_jaccard_no_postprocessing: 0.592519
validation loss: 0.620785 eval_jaccard: 0.709223 eval_jaccard_postprocessing: 0.972674 eval_jaccard_no_postprocessing: 0.522950
Epoch5

lr: 0.000039 train loss: 0.604821 train_jaccard: 0.762194 train_jaccard_postprocessing: 0.970210 train_jaccard_no_postprocessing: 0.610612
lr: 0.000037 train loss: 0.634438 train_jaccard: 0.754956 train_jaccard_postprocessing: 0.969679 train_jaccard_no_postprocessing: 0.604676
lr: 0.000036 train loss: 0.581322 train_jaccard: 0.758684 train_jaccard_postprocessing: 0.971380 train_jaccard_no_postprocessing: 0.608838
validation loss: 0.639626 eval_jaccard: 0.713614 eval_jaccard_postprocessing: 0.972674 eval_jaccard_no_postprocessing: 0.530445
Validation metric improved (0.712605 --> 0.713614).  Saving model ...Model saved as /media/jionie/my_disk/Kaggle/Tweet/model/TweetBert/roberta-base-42/fold_3/7328_step_5_epoch.pth.lr: 0.000035 train loss: 0.568259 train_jaccard: 0.762734 train_jaccard_postprocessing: 0.971144 train_jaccard_no_postprocessing: 0.613684
lr: 0.000033 train loss: 0.606038 train_jaccard: 0.762508 train_jaccard_postprocessing: 0.970325 train_jaccard_no_postprocessing: 0.614550
lr: 0.000032 train loss: 0.598941 train_jaccard: 0.762816 train_jaccard_postprocessing: 0.970459 train_jaccard_no_postprocessing: 0.614222
validation loss: 0.635489 eval_jaccard: 0.712859 eval_jaccard_postprocessing: 0.972674 eval_jaccard_no_postprocessing: 0.529156
lr: 0.000031 train loss: 0.614925 train_jaccard: 0.761553 train_jaccard_postprocessing: 0.970554 train_jaccard_no_postprocessing: 0.612800
lr: 0.000029 train loss: 0.580388 train_jaccard: 0.762065 train_jaccard_postprocessing: 0.970626 train_jaccard_no_postprocessing: 0.612416
lr: 0.000028 train loss: 0.630038 train_jaccard: 0.762187 train_jaccard_postprocessing: 0.970694 train_jaccard_no_postprocessing: 0.613532
lr: 0.000027 train loss: 0.609097 train_jaccard: 0.761974 train_jaccard_postprocessing: 0.970433 train_jaccard_no_postprocessing: 0.613139
validation loss: 0.640013 eval_jaccard: 0.713317 eval_jaccard_postprocessing: 0.972674 eval_jaccard_no_postprocessing: 0.529938
Epoch6

lr: 0.000025 train loss: 0.562395 train_jaccard: 0.774022 train_jaccard_postprocessing: 0.975230 train_jaccard_no_postprocessing: 0.628766
lr: 0.000024 train loss: 0.540111 train_jaccard: 0.774308 train_jaccard_postprocessing: 0.976860 train_jaccard_no_postprocessing: 0.627808
lr: 0.000023 train loss: 0.554656 train_jaccard: 0.774074 train_jaccard_postprocessing: 0.976317 train_jaccard_no_postprocessing: 0.626882
validation loss: 0.650394 eval_jaccard: 0.711719 eval_jaccard_postprocessing: 0.972674 eval_jaccard_no_postprocessing: 0.527210
lr: 0.000021 train loss: 0.563191 train_jaccard: 0.773358 train_jaccard_postprocessing: 0.974294 train_jaccard_no_postprocessing: 0.626661
lr: 0.000020 train loss: 0.587160 train_jaccard: 0.771693 train_jaccard_postprocessing: 0.971879 train_jaccard_no_postprocessing: 0.627446
lr: 0.000019 train loss: 0.591406 train_jaccard: 0.769415 train_jaccard_postprocessing: 0.971555 train_jaccard_no_postprocessing: 0.624804
validation loss: 0.653500 eval_jaccard: 0.709554 eval_jaccard_postprocessing: 0.972674 eval_jaccard_no_postprocessing: 0.523514
lr: 0.000017 train loss: 0.552610 train_jaccard: 0.771119 train_jaccard_postprocessing: 0.971835 train_jaccard_no_postprocessing: 0.626412
lr: 0.000016 train loss: 0.579654 train_jaccard: 0.771784 train_jaccard_postprocessing: 0.971404 train_jaccard_no_postprocessing: 0.628618
lr: 0.000015 train loss: 0.596037 train_jaccard: 0.771198 train_jaccard_postprocessing: 0.970083 train_jaccard_no_postprocessing: 0.628722
lr: 0.000013 train loss: 0.585808 train_jaccard: 0.771848 train_jaccard_postprocessing: 0.970492 train_jaccard_no_postprocessing: 0.629969
validation loss: 0.656814 eval_jaccard: 0.712163 eval_jaccard_postprocessing: 0.972674 eval_jaccard_no_postprocessing: 0.527968
Epoch7

lr: 0.000012 train loss: 0.565265 train_jaccard: 0.773598 train_jaccard_postprocessing: 0.956234 train_jaccard_no_postprocessing: 0.635193
lr: 0.000011 train loss: 0.532269 train_jaccard: 0.779370 train_jaccard_postprocessing: 0.963874 train_jaccard_no_postprocessing: 0.640457
lr: 0.000009 train loss: 0.541365 train_jaccard: 0.781058 train_jaccard_postprocessing: 0.967404 train_jaccard_no_postprocessing: 0.639404
validation loss: 0.661614 eval_jaccard: 0.712671 eval_jaccard_postprocessing: 0.972674 eval_jaccard_no_postprocessing: 0.528835
lr: 0.000008 train loss: 0.560601 train_jaccard: 0.779144 train_jaccard_postprocessing: 0.967886 train_jaccard_no_postprocessing: 0.640702
lr: 0.000007 train loss: 0.563175 train_jaccard: 0.778062 train_jaccard_postprocessing: 0.969538 train_jaccard_no_postprocessing: 0.640195
lr: 0.000005 train loss: 0.568551 train_jaccard: 0.776229 train_jaccard_postprocessing: 0.970470 train_jaccard_no_postprocessing: 0.638610
validation loss: 0.661748 eval_jaccard: 0.712091 eval_jaccard_postprocessing: 0.972674 eval_jaccard_no_postprocessing: 0.527845
lr: 0.000004 train loss: 0.540524 train_jaccard: 0.776879 train_jaccard_postprocessing: 0.970699 train_jaccard_no_postprocessing: 0.638931
lr: 0.000003 train loss: 0.536696 train_jaccard: 0.778102 train_jaccard_postprocessing: 0.970301 train_jaccard_no_postprocessing: 0.640516
lr: 0.000001 train loss: 0.552630 train_jaccard: 0.778254 train_jaccard_postprocessing: 0.970382 train_jaccard_no_postprocessing: 0.640275
lr: 0.000000 train loss: 0.554020 train_jaccard: 0.777748 train_jaccard_postprocessing: 0.970339 train_jaccard_no_postprocessing: 0.640166
validation loss: 0.663332 eval_jaccard: 0.711607 eval_jaccard_postprocessing: 0.972674 eval_jaccard_no_postprocessing: 0.527018
Epoch8

lr: 0.000000 train loss: 0.552538 train_jaccard: 0.768358 train_jaccard_postprocessing: 0.967750 train_jaccard_no_postprocessing: 0.629986
lr: 0.000000 train loss: 0.533699 train_jaccard: 0.776279 train_jaccard_postprocessing: 0.966927 train_jaccard_no_postprocessing: 0.641712
lr: 0.000000 train loss: 0.529905 train_jaccard: 0.782063 train_jaccard_postprocessing: 0.967569 train_jaccard_no_postprocessing: 0.653246
validation loss: 0.663332 eval_jaccard: 0.711607 eval_jaccard_postprocessing: 0.972674 eval_jaccard_no_postprocessing: 0.527018
lr: 0.000000 train loss: 0.548786 train_jaccard: 0.782852 train_jaccard_postprocessing: 0.968149 train_jaccard_no_postprocessing: 0.649708
lr: 0.000000 train loss: 0.559227 train_jaccard: 0.782219 train_jaccard_postprocessing: 0.968079 train_jaccard_no_postprocessing: 0.649645
lr: 0.000000 train loss: 0.538005 train_jaccard: 0.782488 train_jaccard_postprocessing: 0.969040 train_jaccard_no_postprocessing: 0.648903
validation loss: 0.663332 eval_jaccard: 0.711607 eval_jaccard_postprocessing: 0.972674 eval_jaccard_no_postprocessing: 0.527018
lr: 0.000000 train loss: 0.550340 train_jaccard: 0.782056 train_jaccard_postprocessing: 0.969504 train_jaccard_no_postprocessing: 0.648535
lr: 0.000000 train loss: 0.539708 train_jaccard: 0.782230 train_jaccard_postprocessing: 0.969871 train_jaccard_no_postprocessing: 0.647529
lr: 0.000000 train loss: 0.553830 train_jaccard: 0.781163 train_jaccard_postprocessing: 0.969971 train_jaccard_no_postprocessing: 0.645680
lr: 0.000000 train loss: 0.562887 train_jaccard: 0.780731 train_jaccard_postprocessing: 0.970451 train_jaccard_no_postprocessing: 0.645150
validation loss: 0.663332 eval_jaccard: 0.711607 eval_jaccard_postprocessing: 0.972674 eval_jaccard_no_postprocessing: 0.527018
