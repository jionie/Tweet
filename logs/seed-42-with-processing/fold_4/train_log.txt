
Loading data...
*General Setting*
seed: 42
model: TweetBert
trainable parameters:129,374,217
model's state_dict:
device: cuda
use gpu: True
device num: 2
optimizer: AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-08
    initial_lr: 1e-05
    lr: 0.0
    weight_decay: 0.001

Parameter Group 1
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-08
    initial_lr: 0.0002
    lr: 0.0
    weight_decay: 0.001

Parameter Group 2
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-08
    initial_lr: 1e-05
    lr: 0.0
    weight_decay: 0.0

Parameter Group 3
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-08
    initial_lr: 0.0002
    lr: 0.0
    weight_decay: 0.0
)
reuse model: False
adversarial training: False
** start training here! **
   batch_size=16,  accumulation_steps=1
   experiment  = ['training-k-fold-bert-v2.py']
Epoch0

lr: 0.000071 train loss: 2.735077 train_jaccard: 0.527299 train_jaccard_postprocessing: 0.969433 train_jaccard_no_postprocessing: 0.213460
lr: 0.000104 train loss: 1.354928 train_jaccard: 0.566578 train_jaccard_postprocessing: 0.973043 train_jaccard_no_postprocessing: 0.276971
lr: 0.000103 train loss: 1.181897 train_jaccard: 0.589985 train_jaccard_postprocessing: 0.970322 train_jaccard_no_postprocessing: 0.311619
validation loss: 0.877019 eval_jaccard: 0.651224 eval_jaccard_postprocessing: 0.974355 eval_jaccard_no_postprocessing: 0.423096
Validation metric improved (-inf --> 0.651224).  Saving model ...Model saved as /media/jionie/my_disk/Kaggle/Tweet/model/TweetBert/roberta-base-42/fold_4/458_step_0_epoch.pth.lr: 0.000102 train loss: 1.052052 train_jaccard: 0.605153 train_jaccard_postprocessing: 0.969555 train_jaccard_no_postprocessing: 0.336114
lr: 0.000100 train loss: 0.987561 train_jaccard: 0.615718 train_jaccard_postprocessing: 0.968933 train_jaccard_no_postprocessing: 0.358617
lr: 0.000099 train loss: 0.915276 train_jaccard: 0.625723 train_jaccard_postprocessing: 0.969732 train_jaccard_no_postprocessing: 0.377223
validation loss: 0.679510 eval_jaccard: 0.694880 eval_jaccard_postprocessing: 0.974355 eval_jaccard_no_postprocessing: 0.497573
Validation metric improved (0.651224 --> 0.694880).  Saving model ...Model saved as /media/jionie/my_disk/Kaggle/Tweet/model/TweetBert/roberta-base-42/fold_4/916_step_0_epoch.pth.lr: 0.000098 train loss: 0.907389 train_jaccard: 0.632324 train_jaccard_postprocessing: 0.969368 train_jaccard_no_postprocessing: 0.390500
lr: 0.000096 train loss: 0.874352 train_jaccard: 0.639528 train_jaccard_postprocessing: 0.969326 train_jaccard_no_postprocessing: 0.401497
lr: 0.000095 train loss: 0.866186 train_jaccard: 0.645707 train_jaccard_postprocessing: 0.970186 train_jaccard_no_postprocessing: 0.412239
lr: 0.000094 train loss: 0.870233 train_jaccard: 0.649395 train_jaccard_postprocessing: 0.969970 train_jaccard_no_postprocessing: 0.420341
validation loss: 0.656661 eval_jaccard: 0.700317 eval_jaccard_postprocessing: 0.974355 eval_jaccard_no_postprocessing: 0.506848
Validation metric improved (0.694880 --> 0.700317).  Saving model ...Model saved as /media/jionie/my_disk/Kaggle/Tweet/model/TweetBert/roberta-base-42/fold_4/1374_step_0_epoch.pth.Epoch1

lr: 0.000092 train loss: 0.827844 train_jaccard: 0.711034 train_jaccard_postprocessing: 0.963176 train_jaccard_no_postprocessing: 0.523832
lr: 0.000091 train loss: 0.810710 train_jaccard: 0.704949 train_jaccard_postprocessing: 0.966279 train_jaccard_no_postprocessing: 0.512907
lr: 0.000090 train loss: 0.850878 train_jaccard: 0.701206 train_jaccard_postprocessing: 0.966257 train_jaccard_no_postprocessing: 0.513418
validation loss: 0.659225 eval_jaccard: 0.701506 eval_jaccard_postprocessing: 0.974355 eval_jaccard_no_postprocessing: 0.508876
Validation metric improved (0.700317 --> 0.701506).  Saving model ...Model saved as /media/jionie/my_disk/Kaggle/Tweet/model/TweetBert/roberta-base-42/fold_4/1832_step_1_epoch.pth.lr: 0.000088 train loss: 0.780881 train_jaccard: 0.703541 train_jaccard_postprocessing: 0.967448 train_jaccard_no_postprocessing: 0.517265
lr: 0.000087 train loss: 0.822064 train_jaccard: 0.700231 train_jaccard_postprocessing: 0.969064 train_jaccard_no_postprocessing: 0.511831
lr: 0.000086 train loss: 0.827719 train_jaccard: 0.700142 train_jaccard_postprocessing: 0.969868 train_jaccard_no_postprocessing: 0.510355
validation loss: 0.641159 eval_jaccard: 0.707442 eval_jaccard_postprocessing: 0.974355 eval_jaccard_no_postprocessing: 0.519003
Validation metric improved (0.701506 --> 0.707442).  Saving model ...Model saved as /media/jionie/my_disk/Kaggle/Tweet/model/TweetBert/roberta-base-42/fold_4/2290_step_1_epoch.pth.lr: 0.000084 train loss: 0.781580 train_jaccard: 0.702842 train_jaccard_postprocessing: 0.969583 train_jaccard_no_postprocessing: 0.512943
lr: 0.000083 train loss: 0.771895 train_jaccard: 0.705507 train_jaccard_postprocessing: 0.970056 train_jaccard_no_postprocessing: 0.515640
lr: 0.000082 train loss: 0.830235 train_jaccard: 0.704717 train_jaccard_postprocessing: 0.969388 train_jaccard_no_postprocessing: 0.514955
lr: 0.000080 train loss: 0.793130 train_jaccard: 0.704451 train_jaccard_postprocessing: 0.969964 train_jaccard_no_postprocessing: 0.514633
validation loss: 0.642391 eval_jaccard: 0.706492 eval_jaccard_postprocessing: 0.974355 eval_jaccard_no_postprocessing: 0.517382
Epoch2

lr: 0.000079 train loss: 0.755999 train_jaccard: 0.723325 train_jaccard_postprocessing: 0.971165 train_jaccard_no_postprocessing: 0.556142
lr: 0.000078 train loss: 0.742954 train_jaccard: 0.724981 train_jaccard_postprocessing: 0.968475 train_jaccard_no_postprocessing: 0.553759
lr: 0.000076 train loss: 0.744557 train_jaccard: 0.726881 train_jaccard_postprocessing: 0.969505 train_jaccard_no_postprocessing: 0.551617
validation loss: 0.645380 eval_jaccard: 0.711831 eval_jaccard_postprocessing: 0.974355 eval_jaccard_no_postprocessing: 0.526491
Validation metric improved (0.707442 --> 0.711831).  Saving model ...Model saved as /media/jionie/my_disk/Kaggle/Tweet/model/TweetBert/roberta-base-42/fold_4/3206_step_2_epoch.pth.lr: 0.000075 train loss: 0.744128 train_jaccard: 0.727068 train_jaccard_postprocessing: 0.969742 train_jaccard_no_postprocessing: 0.552859
lr: 0.000074 train loss: 0.771795 train_jaccard: 0.724632 train_jaccard_postprocessing: 0.969277 train_jaccard_no_postprocessing: 0.549600
lr: 0.000072 train loss: 0.744658 train_jaccard: 0.724504 train_jaccard_postprocessing: 0.969705 train_jaccard_no_postprocessing: 0.548867
validation loss: 0.647446 eval_jaccard: 0.706851 eval_jaccard_postprocessing: 0.974355 eval_jaccard_no_postprocessing: 0.517994
lr: 0.000071 train loss: 0.754416 train_jaccard: 0.723154 train_jaccard_postprocessing: 0.969784 train_jaccard_no_postprocessing: 0.546627
lr: 0.000070 train loss: 0.750432 train_jaccard: 0.722873 train_jaccard_postprocessing: 0.970261 train_jaccard_no_postprocessing: 0.546153
lr: 0.000068 train loss: 0.763888 train_jaccard: 0.721812 train_jaccard_postprocessing: 0.970329 train_jaccard_no_postprocessing: 0.544817
lr: 0.000067 train loss: 0.766224 train_jaccard: 0.721779 train_jaccard_postprocessing: 0.969936 train_jaccard_no_postprocessing: 0.544301
validation loss: 0.636553 eval_jaccard: 0.710892 eval_jaccard_postprocessing: 0.974355 eval_jaccard_no_postprocessing: 0.524888
Epoch3

lr: 0.000066 train loss: 0.695102 train_jaccard: 0.724877 train_jaccard_postprocessing: 0.973089 train_jaccard_no_postprocessing: 0.551648
lr: 0.000064 train loss: 0.711029 train_jaccard: 0.727980 train_jaccard_postprocessing: 0.971817 train_jaccard_no_postprocessing: 0.558924
lr: 0.000063 train loss: 0.710291 train_jaccard: 0.728183 train_jaccard_postprocessing: 0.973017 train_jaccard_no_postprocessing: 0.559659
validation loss: 0.653605 eval_jaccard: 0.706899 eval_jaccard_postprocessing: 0.974355 eval_jaccard_no_postprocessing: 0.518077
lr: 0.000062 train loss: 0.701818 train_jaccard: 0.729467 train_jaccard_postprocessing: 0.971962 train_jaccard_no_postprocessing: 0.559588
lr: 0.000060 train loss: 0.697786 train_jaccard: 0.730633 train_jaccard_postprocessing: 0.971066 train_jaccard_no_postprocessing: 0.560350
lr: 0.000059 train loss: 0.716553 train_jaccard: 0.730266 train_jaccard_postprocessing: 0.969574 train_jaccard_no_postprocessing: 0.560929
validation loss: 0.651328 eval_jaccard: 0.712016 eval_jaccard_postprocessing: 0.974355 eval_jaccard_no_postprocessing: 0.526807
Validation metric improved (0.711831 --> 0.712016).  Saving model ...Model saved as /media/jionie/my_disk/Kaggle/Tweet/model/TweetBert/roberta-base-42/fold_4/5038_step_3_epoch.pth.lr: 0.000058 train loss: 0.721300 train_jaccard: 0.730142 train_jaccard_postprocessing: 0.969784 train_jaccard_no_postprocessing: 0.561448
lr: 0.000056 train loss: 0.699437 train_jaccard: 0.730895 train_jaccard_postprocessing: 0.969733 train_jaccard_no_postprocessing: 0.561600
lr: 0.000055 train loss: 0.688384 train_jaccard: 0.731791 train_jaccard_postprocessing: 0.969944 train_jaccard_no_postprocessing: 0.562319
lr: 0.000054 train loss: 0.693129 train_jaccard: 0.733472 train_jaccard_postprocessing: 0.970007 train_jaccard_no_postprocessing: 0.564402
validation loss: 0.648312 eval_jaccard: 0.712207 eval_jaccard_postprocessing: 0.974355 eval_jaccard_no_postprocessing: 0.527133
Validation metric improved (0.712016 --> 0.712207).  Saving model ...Model saved as /media/jionie/my_disk/Kaggle/Tweet/model/TweetBert/roberta-base-42/fold_4/5496_step_3_epoch.pth.Epoch4

lr: 0.000052 train loss: 0.645701 train_jaccard: 0.753138 train_jaccard_postprocessing: 0.973157 train_jaccard_no_postprocessing: 0.590696
lr: 0.000051 train loss: 0.660480 train_jaccard: 0.745655 train_jaccard_postprocessing: 0.973588 train_jaccard_no_postprocessing: 0.581875
lr: 0.000049 train loss: 0.676703 train_jaccard: 0.746438 train_jaccard_postprocessing: 0.971338 train_jaccard_no_postprocessing: 0.584078
validation loss: 0.665541 eval_jaccard: 0.712317 eval_jaccard_postprocessing: 0.974355 eval_jaccard_no_postprocessing: 0.527320
Validation metric improved (0.712207 --> 0.712317).  Saving model ...Model saved as /media/jionie/my_disk/Kaggle/Tweet/model/TweetBert/roberta-base-42/fold_4/5954_step_4_epoch.pth.lr: 0.000048 train loss: 0.640332 train_jaccard: 0.750000 train_jaccard_postprocessing: 0.971904 train_jaccard_no_postprocessing: 0.589578
lr: 0.000047 train loss: 0.662568 train_jaccard: 0.749505 train_jaccard_postprocessing: 0.971549 train_jaccard_no_postprocessing: 0.591061
lr: 0.000045 train loss: 0.665138 train_jaccard: 0.749718 train_jaccard_postprocessing: 0.971309 train_jaccard_no_postprocessing: 0.591291
validation loss: 0.658552 eval_jaccard: 0.710144 eval_jaccard_postprocessing: 0.974355 eval_jaccard_no_postprocessing: 0.523613
lr: 0.000044 train loss: 0.663348 train_jaccard: 0.749870 train_jaccard_postprocessing: 0.970901 train_jaccard_no_postprocessing: 0.591369
lr: 0.000043 train loss: 0.643071 train_jaccard: 0.750741 train_jaccard_postprocessing: 0.970717 train_jaccard_no_postprocessing: 0.592085
lr: 0.000041 train loss: 0.669149 train_jaccard: 0.750211 train_jaccard_postprocessing: 0.970394 train_jaccard_no_postprocessing: 0.592247
lr: 0.000040 train loss: 0.680892 train_jaccard: 0.749606 train_jaccard_postprocessing: 0.970023 train_jaccard_no_postprocessing: 0.592324
validation loss: 0.653637 eval_jaccard: 0.710422 eval_jaccard_postprocessing: 0.974355 eval_jaccard_no_postprocessing: 0.524086
Epoch5

lr: 0.000039 train loss: 0.615904 train_jaccard: 0.761551 train_jaccard_postprocessing: 0.974314 train_jaccard_no_postprocessing: 0.611939
lr: 0.000037 train loss: 0.636756 train_jaccard: 0.757508 train_jaccard_postprocessing: 0.973222 train_jaccard_no_postprocessing: 0.611028
lr: 0.000036 train loss: 0.623043 train_jaccard: 0.756060 train_jaccard_postprocessing: 0.973493 train_jaccard_no_postprocessing: 0.604787
validation loss: 0.675179 eval_jaccard: 0.707280 eval_jaccard_postprocessing: 0.974355 eval_jaccard_no_postprocessing: 0.518726
lr: 0.000035 train loss: 0.619796 train_jaccard: 0.757296 train_jaccard_postprocessing: 0.972519 train_jaccard_no_postprocessing: 0.606098
lr: 0.000033 train loss: 0.623637 train_jaccard: 0.758212 train_jaccard_postprocessing: 0.970611 train_jaccard_no_postprocessing: 0.608238
lr: 0.000032 train loss: 0.657406 train_jaccard: 0.758176 train_jaccard_postprocessing: 0.969625 train_jaccard_no_postprocessing: 0.609768
validation loss: 0.666764 eval_jaccard: 0.709481 eval_jaccard_postprocessing: 0.974355 eval_jaccard_no_postprocessing: 0.522482
lr: 0.000031 train loss: 0.641307 train_jaccard: 0.758737 train_jaccard_postprocessing: 0.969627 train_jaccard_no_postprocessing: 0.610442
lr: 0.000029 train loss: 0.612962 train_jaccard: 0.759529 train_jaccard_postprocessing: 0.969807 train_jaccard_no_postprocessing: 0.610689
lr: 0.000028 train loss: 0.628180 train_jaccard: 0.759129 train_jaccard_postprocessing: 0.970126 train_jaccard_no_postprocessing: 0.609638
lr: 0.000027 train loss: 0.631623 train_jaccard: 0.758833 train_jaccard_postprocessing: 0.969926 train_jaccard_no_postprocessing: 0.608175
validation loss: 0.666216 eval_jaccard: 0.710784 eval_jaccard_postprocessing: 0.974355 eval_jaccard_no_postprocessing: 0.524704
Epoch6

lr: 0.000025 train loss: 0.578127 train_jaccard: 0.778255 train_jaccard_postprocessing: 0.968340 train_jaccard_no_postprocessing: 0.634205
lr: 0.000024 train loss: 0.609978 train_jaccard: 0.771302 train_jaccard_postprocessing: 0.968080 train_jaccard_no_postprocessing: 0.630438
lr: 0.000023 train loss: 0.603804 train_jaccard: 0.772282 train_jaccard_postprocessing: 0.970271 train_jaccard_no_postprocessing: 0.632095
validation loss: 0.691200 eval_jaccard: 0.711545 eval_jaccard_postprocessing: 0.974355 eval_jaccard_no_postprocessing: 0.526002
lr: 0.000021 train loss: 0.591433 train_jaccard: 0.774207 train_jaccard_postprocessing: 0.970021 train_jaccard_no_postprocessing: 0.636383
lr: 0.000020 train loss: 0.617634 train_jaccard: 0.771882 train_jaccard_postprocessing: 0.969905 train_jaccard_no_postprocessing: 0.632687
lr: 0.000019 train loss: 0.588461 train_jaccard: 0.771334 train_jaccard_postprocessing: 0.970690 train_jaccard_no_postprocessing: 0.630797
validation loss: 0.681684 eval_jaccard: 0.709436 eval_jaccard_postprocessing: 0.974355 eval_jaccard_no_postprocessing: 0.522405
lr: 0.000017 train loss: 0.561315 train_jaccard: 0.772685 train_jaccard_postprocessing: 0.969946 train_jaccard_no_postprocessing: 0.631683
lr: 0.000016 train loss: 0.588461 train_jaccard: 0.772572 train_jaccard_postprocessing: 0.969880 train_jaccard_no_postprocessing: 0.630963
lr: 0.000015 train loss: 0.613018 train_jaccard: 0.771463 train_jaccard_postprocessing: 0.970065 train_jaccard_no_postprocessing: 0.629634
lr: 0.000013 train loss: 0.621193 train_jaccard: 0.770313 train_jaccard_postprocessing: 0.969929 train_jaccard_no_postprocessing: 0.627713
validation loss: 0.682785 eval_jaccard: 0.708516 eval_jaccard_postprocessing: 0.974355 eval_jaccard_no_postprocessing: 0.520836
Epoch7

lr: 0.000012 train loss: 0.582694 train_jaccard: 0.773527 train_jaccard_postprocessing: 0.971581 train_jaccard_no_postprocessing: 0.630012
lr: 0.000011 train loss: 0.571706 train_jaccard: 0.776296 train_jaccard_postprocessing: 0.972004 train_jaccard_no_postprocessing: 0.638158
lr: 0.000009 train loss: 0.589671 train_jaccard: 0.774545 train_jaccard_postprocessing: 0.971139 train_jaccard_no_postprocessing: 0.634648
validation loss: 0.695483 eval_jaccard: 0.709658 eval_jaccard_postprocessing: 0.974355 eval_jaccard_no_postprocessing: 0.522783
lr: 0.000008 train loss: 0.572104 train_jaccard: 0.777226 train_jaccard_postprocessing: 0.971731 train_jaccard_no_postprocessing: 0.638381
lr: 0.000007 train loss: 0.580104 train_jaccard: 0.777218 train_jaccard_postprocessing: 0.971136 train_jaccard_no_postprocessing: 0.638999
lr: 0.000005 train loss: 0.560824 train_jaccard: 0.778726 train_jaccard_postprocessing: 0.969842 train_jaccard_no_postprocessing: 0.641702
validation loss: 0.695802 eval_jaccard: 0.709254 eval_jaccard_postprocessing: 0.974355 eval_jaccard_no_postprocessing: 0.522095
lr: 0.000004 train loss: 0.604131 train_jaccard: 0.777046 train_jaccard_postprocessing: 0.969042 train_jaccard_no_postprocessing: 0.639623
lr: 0.000003 train loss: 0.595921 train_jaccard: 0.776424 train_jaccard_postprocessing: 0.969007 train_jaccard_no_postprocessing: 0.639980
lr: 0.000001 train loss: 0.551193 train_jaccard: 0.777617 train_jaccard_postprocessing: 0.969301 train_jaccard_no_postprocessing: 0.641042
lr: 0.000000 train loss: 0.582599 train_jaccard: 0.778066 train_jaccard_postprocessing: 0.969909 train_jaccard_no_postprocessing: 0.641146
validation loss: 0.699221 eval_jaccard: 0.708202 eval_jaccard_postprocessing: 0.974355 eval_jaccard_no_postprocessing: 0.520299
