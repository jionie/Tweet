
Loading data...
*General Setting*
seed: 42
model: TweetBert
trainable parameters:129,374,217
model's state_dict:
device: cuda
use gpu: True
device num: 2
optimizer: AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-08
    initial_lr: 1e-05
    lr: 0.0
    weight_decay: 0.001

Parameter Group 1
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-08
    initial_lr: 0.0002
    lr: 0.0
    weight_decay: 0.001

Parameter Group 2
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-08
    initial_lr: 1e-05
    lr: 0.0
    weight_decay: 0.0

Parameter Group 3
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-08
    initial_lr: 0.0002
    lr: 0.0
    weight_decay: 0.0
)
reuse model: False
adversarial training: False
** start training here! **
   batch_size=16,  accumulation_steps=1
   experiment  = ['training-k-fold-bert-v2.py']
Epoch0

lr: 0.000071 train loss: 2.715718 train_jaccard: 0.547500 train_jaccard_postprocessing: 0.974906 train_jaccard_no_postprocessing: 0.199990
lr: 0.000104 train loss: 1.372184 train_jaccard: 0.574088 train_jaccard_postprocessing: 0.973497 train_jaccard_no_postprocessing: 0.276440
lr: 0.000103 train loss: 1.189290 train_jaccard: 0.592707 train_jaccard_postprocessing: 0.971813 train_jaccard_no_postprocessing: 0.313855
validation loss: 0.778555 eval_jaccard: 0.673096 eval_jaccard_postprocessing: 0.971712 eval_jaccard_no_postprocessing: 0.459226
Validation metric improved (-inf --> 0.673096).  Saving model ...Model saved as /media/jionie/my_disk/Kaggle/Tweet/model/TweetBert/roberta-base-42/fold_1/458_step_0_epoch.pth.lr: 0.000102 train loss: 1.017762 train_jaccard: 0.609718 train_jaccard_postprocessing: 0.972082 train_jaccard_no_postprocessing: 0.350441
lr: 0.000100 train loss: 0.912988 train_jaccard: 0.623781 train_jaccard_postprocessing: 0.971431 train_jaccard_no_postprocessing: 0.373745
lr: 0.000099 train loss: 0.925293 train_jaccard: 0.632479 train_jaccard_postprocessing: 0.970889 train_jaccard_no_postprocessing: 0.389392
validation loss: 0.679288 eval_jaccard: 0.691761 eval_jaccard_postprocessing: 0.971712 eval_jaccard_no_postprocessing: 0.491259
Validation metric improved (0.673096 --> 0.691761).  Saving model ...Model saved as /media/jionie/my_disk/Kaggle/Tweet/model/TweetBert/roberta-base-42/fold_1/916_step_0_epoch.pth.lr: 0.000098 train loss: 0.916356 train_jaccard: 0.637603 train_jaccard_postprocessing: 0.970096 train_jaccard_no_postprocessing: 0.401146
lr: 0.000096 train loss: 0.895181 train_jaccard: 0.643640 train_jaccard_postprocessing: 0.970622 train_jaccard_no_postprocessing: 0.411102
lr: 0.000095 train loss: 0.880317 train_jaccard: 0.648415 train_jaccard_postprocessing: 0.970742 train_jaccard_no_postprocessing: 0.418710
lr: 0.000094 train loss: 0.853635 train_jaccard: 0.653059 train_jaccard_postprocessing: 0.970754 train_jaccard_no_postprocessing: 0.426828
validation loss: 0.647095 eval_jaccard: 0.696320 eval_jaccard_postprocessing: 0.971712 eval_jaccard_no_postprocessing: 0.499083
Validation metric improved (0.691761 --> 0.696320).  Saving model ...Model saved as /media/jionie/my_disk/Kaggle/Tweet/model/TweetBert/roberta-base-42/fold_1/1374_step_0_epoch.pth.Epoch1

lr: 0.000092 train loss: 0.815214 train_jaccard: 0.706692 train_jaccard_postprocessing: 0.970732 train_jaccard_no_postprocessing: 0.515720
lr: 0.000091 train loss: 0.825416 train_jaccard: 0.700109 train_jaccard_postprocessing: 0.970289 train_jaccard_no_postprocessing: 0.508689
lr: 0.000090 train loss: 0.851539 train_jaccard: 0.693868 train_jaccard_postprocessing: 0.968699 train_jaccard_no_postprocessing: 0.505292
validation loss: 0.640435 eval_jaccard: 0.702815 eval_jaccard_postprocessing: 0.971712 eval_jaccard_no_postprocessing: 0.510230
Validation metric improved (0.696320 --> 0.702815).  Saving model ...Model saved as /media/jionie/my_disk/Kaggle/Tweet/model/TweetBert/roberta-base-42/fold_1/1832_step_1_epoch.pth.lr: 0.000088 train loss: 0.822266 train_jaccard: 0.693492 train_jaccard_postprocessing: 0.970258 train_jaccard_no_postprocessing: 0.503587
lr: 0.000087 train loss: 0.784828 train_jaccard: 0.699387 train_jaccard_postprocessing: 0.969518 train_jaccard_no_postprocessing: 0.509935
lr: 0.000086 train loss: 0.806720 train_jaccard: 0.699699 train_jaccard_postprocessing: 0.970043 train_jaccard_no_postprocessing: 0.510251
validation loss: 0.639927 eval_jaccard: 0.703776 eval_jaccard_postprocessing: 0.971712 eval_jaccard_no_postprocessing: 0.511880
Validation metric improved (0.702815 --> 0.703776).  Saving model ...Model saved as /media/jionie/my_disk/Kaggle/Tweet/model/TweetBert/roberta-base-42/fold_1/2290_step_1_epoch.pth.lr: 0.000084 train loss: 0.782102 train_jaccard: 0.702663 train_jaccard_postprocessing: 0.970349 train_jaccard_no_postprocessing: 0.512651
lr: 0.000083 train loss: 0.822703 train_jaccard: 0.703022 train_jaccard_postprocessing: 0.970057 train_jaccard_no_postprocessing: 0.512982
lr: 0.000082 train loss: 0.807891 train_jaccard: 0.703616 train_jaccard_postprocessing: 0.970199 train_jaccard_no_postprocessing: 0.513835
lr: 0.000080 train loss: 0.758883 train_jaccard: 0.705038 train_jaccard_postprocessing: 0.970615 train_jaccard_no_postprocessing: 0.516204
validation loss: 0.632604 eval_jaccard: 0.701263 eval_jaccard_postprocessing: 0.971712 eval_jaccard_no_postprocessing: 0.507566
Epoch2

lr: 0.000079 train loss: 0.766182 train_jaccard: 0.716737 train_jaccard_postprocessing: 0.966817 train_jaccard_no_postprocessing: 0.539557
lr: 0.000078 train loss: 0.737036 train_jaccard: 0.721771 train_jaccard_postprocessing: 0.969088 train_jaccard_no_postprocessing: 0.545225
lr: 0.000076 train loss: 0.757541 train_jaccard: 0.718791 train_jaccard_postprocessing: 0.968098 train_jaccard_no_postprocessing: 0.540044
validation loss: 0.630263 eval_jaccard: 0.702275 eval_jaccard_postprocessing: 0.971712 eval_jaccard_no_postprocessing: 0.509303
lr: 0.000075 train loss: 0.767966 train_jaccard: 0.718247 train_jaccard_postprocessing: 0.969654 train_jaccard_no_postprocessing: 0.538530
lr: 0.000074 train loss: 0.719465 train_jaccard: 0.720730 train_jaccard_postprocessing: 0.969294 train_jaccard_no_postprocessing: 0.541758
lr: 0.000072 train loss: 0.733357 train_jaccard: 0.721758 train_jaccard_postprocessing: 0.971033 train_jaccard_no_postprocessing: 0.542755
validation loss: 0.633427 eval_jaccard: 0.701896 eval_jaccard_postprocessing: 0.971712 eval_jaccard_no_postprocessing: 0.508652
lr: 0.000071 train loss: 0.730519 train_jaccard: 0.722999 train_jaccard_postprocessing: 0.972146 train_jaccard_no_postprocessing: 0.545100
lr: 0.000070 train loss: 0.762808 train_jaccard: 0.722514 train_jaccard_postprocessing: 0.971998 train_jaccard_no_postprocessing: 0.545548
lr: 0.000068 train loss: 0.744449 train_jaccard: 0.723139 train_jaccard_postprocessing: 0.971412 train_jaccard_no_postprocessing: 0.546208
lr: 0.000067 train loss: 0.759198 train_jaccard: 0.722596 train_jaccard_postprocessing: 0.970601 train_jaccard_no_postprocessing: 0.545893
validation loss: 0.624794 eval_jaccard: 0.703427 eval_jaccard_postprocessing: 0.971712 eval_jaccard_no_postprocessing: 0.511281
Epoch3

lr: 0.000066 train loss: 0.682833 train_jaccard: 0.742484 train_jaccard_postprocessing: 0.970057 train_jaccard_no_postprocessing: 0.577886
lr: 0.000064 train loss: 0.713107 train_jaccard: 0.736218 train_jaccard_postprocessing: 0.969325 train_jaccard_no_postprocessing: 0.567776
lr: 0.000063 train loss: 0.691010 train_jaccard: 0.736028 train_jaccard_postprocessing: 0.970071 train_jaccard_no_postprocessing: 0.569898
validation loss: 0.639379 eval_jaccard: 0.708257 eval_jaccard_postprocessing: 0.971712 eval_jaccard_no_postprocessing: 0.519569
Validation metric improved (0.703776 --> 0.708257).  Saving model ...Model saved as /media/jionie/my_disk/Kaggle/Tweet/model/TweetBert/roberta-base-42/fold_1/4580_step_3_epoch.pth.lr: 0.000062 train loss: 0.714945 train_jaccard: 0.735762 train_jaccard_postprocessing: 0.969220 train_jaccard_no_postprocessing: 0.572446
lr: 0.000060 train loss: 0.685478 train_jaccard: 0.739576 train_jaccard_postprocessing: 0.968812 train_jaccard_no_postprocessing: 0.576061
lr: 0.000059 train loss: 0.665614 train_jaccard: 0.741203 train_jaccard_postprocessing: 0.969561 train_jaccard_no_postprocessing: 0.575677
validation loss: 0.640645 eval_jaccard: 0.709339 eval_jaccard_postprocessing: 0.971712 eval_jaccard_no_postprocessing: 0.521427
Validation metric improved (0.708257 --> 0.709339).  Saving model ...Model saved as /media/jionie/my_disk/Kaggle/Tweet/model/TweetBert/roberta-base-42/fold_1/5038_step_3_epoch.pth.lr: 0.000058 train loss: 0.698481 train_jaccard: 0.740905 train_jaccard_postprocessing: 0.970488 train_jaccard_no_postprocessing: 0.575609
lr: 0.000056 train loss: 0.731238 train_jaccard: 0.738213 train_jaccard_postprocessing: 0.970324 train_jaccard_no_postprocessing: 0.572172
lr: 0.000055 train loss: 0.702753 train_jaccard: 0.737479 train_jaccard_postprocessing: 0.970264 train_jaccard_no_postprocessing: 0.570753
lr: 0.000054 train loss: 0.706552 train_jaccard: 0.736673 train_jaccard_postprocessing: 0.970718 train_jaccard_no_postprocessing: 0.570042
validation loss: 0.634400 eval_jaccard: 0.705961 eval_jaccard_postprocessing: 0.971712 eval_jaccard_no_postprocessing: 0.515629
Epoch4

lr: 0.000052 train loss: 0.652954 train_jaccard: 0.753820 train_jaccard_postprocessing: 0.973434 train_jaccard_no_postprocessing: 0.599391
lr: 0.000051 train loss: 0.624860 train_jaccard: 0.756717 train_jaccard_postprocessing: 0.972750 train_jaccard_no_postprocessing: 0.603802
lr: 0.000049 train loss: 0.665230 train_jaccard: 0.751853 train_jaccard_postprocessing: 0.971856 train_jaccard_no_postprocessing: 0.594807
validation loss: 0.647448 eval_jaccard: 0.706513 eval_jaccard_postprocessing: 0.971712 eval_jaccard_no_postprocessing: 0.516577
lr: 0.000048 train loss: 0.665884 train_jaccard: 0.750753 train_jaccard_postprocessing: 0.971303 train_jaccard_no_postprocessing: 0.593242
lr: 0.000047 train loss: 0.687918 train_jaccard: 0.745965 train_jaccard_postprocessing: 0.971535 train_jaccard_no_postprocessing: 0.587586
lr: 0.000045 train loss: 0.654929 train_jaccard: 0.746285 train_jaccard_postprocessing: 0.971389 train_jaccard_no_postprocessing: 0.589183
validation loss: 0.647177 eval_jaccard: 0.705671 eval_jaccard_postprocessing: 0.971712 eval_jaccard_no_postprocessing: 0.515131
lr: 0.000044 train loss: 0.678791 train_jaccard: 0.746141 train_jaccard_postprocessing: 0.971198 train_jaccard_no_postprocessing: 0.587331
lr: 0.000043 train loss: 0.660966 train_jaccard: 0.746502 train_jaccard_postprocessing: 0.970450 train_jaccard_no_postprocessing: 0.587425
lr: 0.000041 train loss: 0.661660 train_jaccard: 0.746542 train_jaccard_postprocessing: 0.970456 train_jaccard_no_postprocessing: 0.587469
lr: 0.000040 train loss: 0.641937 train_jaccard: 0.747914 train_jaccard_postprocessing: 0.970611 train_jaccard_no_postprocessing: 0.589331
validation loss: 0.652024 eval_jaccard: 0.710289 eval_jaccard_postprocessing: 0.971712 eval_jaccard_no_postprocessing: 0.523057
Validation metric improved (0.709339 --> 0.710289).  Saving model ...Model saved as /media/jionie/my_disk/Kaggle/Tweet/model/TweetBert/roberta-base-42/fold_1/6870_step_4_epoch.pth.Epoch5

lr: 0.000039 train loss: 0.611509 train_jaccard: 0.762383 train_jaccard_postprocessing: 0.968606 train_jaccard_no_postprocessing: 0.615999
lr: 0.000037 train loss: 0.600553 train_jaccard: 0.764172 train_jaccard_postprocessing: 0.969679 train_jaccard_no_postprocessing: 0.612304
lr: 0.000036 train loss: 0.645247 train_jaccard: 0.756891 train_jaccard_postprocessing: 0.970129 train_jaccard_no_postprocessing: 0.605338
validation loss: 0.662735 eval_jaccard: 0.711235 eval_jaccard_postprocessing: 0.971712 eval_jaccard_no_postprocessing: 0.524680
Validation metric improved (0.710289 --> 0.711235).  Saving model ...Model saved as /media/jionie/my_disk/Kaggle/Tweet/model/TweetBert/roberta-base-42/fold_1/7328_step_5_epoch.pth.lr: 0.000035 train loss: 0.624665 train_jaccard: 0.758843 train_jaccard_postprocessing: 0.970776 train_jaccard_no_postprocessing: 0.610374
lr: 0.000033 train loss: 0.635945 train_jaccard: 0.758477 train_jaccard_postprocessing: 0.970601 train_jaccard_no_postprocessing: 0.610156
lr: 0.000032 train loss: 0.602458 train_jaccard: 0.759934 train_jaccard_postprocessing: 0.971109 train_jaccard_no_postprocessing: 0.610506
validation loss: 0.658316 eval_jaccard: 0.705883 eval_jaccard_postprocessing: 0.971712 eval_jaccard_no_postprocessing: 0.515496
lr: 0.000031 train loss: 0.631760 train_jaccard: 0.759474 train_jaccard_postprocessing: 0.970699 train_jaccard_no_postprocessing: 0.610143
lr: 0.000029 train loss: 0.627938 train_jaccard: 0.760293 train_jaccard_postprocessing: 0.970468 train_jaccard_no_postprocessing: 0.611595
lr: 0.000028 train loss: 0.646985 train_jaccard: 0.760148 train_jaccard_postprocessing: 0.969995 train_jaccard_no_postprocessing: 0.612061
lr: 0.000027 train loss: 0.606226 train_jaccard: 0.760496 train_jaccard_postprocessing: 0.970570 train_jaccard_no_postprocessing: 0.611043
validation loss: 0.672432 eval_jaccard: 0.703760 eval_jaccard_postprocessing: 0.971712 eval_jaccard_no_postprocessing: 0.511851
Epoch6

lr: 0.000025 train loss: 0.585291 train_jaccard: 0.786145 train_jaccard_postprocessing: 0.977356 train_jaccard_no_postprocessing: 0.642585
lr: 0.000024 train loss: 0.595840 train_jaccard: 0.781326 train_jaccard_postprocessing: 0.975036 train_jaccard_no_postprocessing: 0.635482
lr: 0.000023 train loss: 0.613861 train_jaccard: 0.775564 train_jaccard_postprocessing: 0.972408 train_jaccard_no_postprocessing: 0.634079
validation loss: 0.678067 eval_jaccard: 0.704530 eval_jaccard_postprocessing: 0.971712 eval_jaccard_no_postprocessing: 0.513173
lr: 0.000021 train loss: 0.611139 train_jaccard: 0.772111 train_jaccard_postprocessing: 0.972360 train_jaccard_no_postprocessing: 0.631629
lr: 0.000020 train loss: 0.580573 train_jaccard: 0.772541 train_jaccard_postprocessing: 0.971749 train_jaccard_no_postprocessing: 0.631138
lr: 0.000019 train loss: 0.599620 train_jaccard: 0.772048 train_jaccard_postprocessing: 0.970474 train_jaccard_no_postprocessing: 0.629915
validation loss: 0.674341 eval_jaccard: 0.707011 eval_jaccard_postprocessing: 0.971712 eval_jaccard_no_postprocessing: 0.517431
lr: 0.000017 train loss: 0.586398 train_jaccard: 0.771960 train_jaccard_postprocessing: 0.969718 train_jaccard_no_postprocessing: 0.631472
lr: 0.000016 train loss: 0.606935 train_jaccard: 0.771513 train_jaccard_postprocessing: 0.970147 train_jaccard_no_postprocessing: 0.631705
lr: 0.000015 train loss: 0.601397 train_jaccard: 0.772159 train_jaccard_postprocessing: 0.969974 train_jaccard_no_postprocessing: 0.632009
lr: 0.000013 train loss: 0.584879 train_jaccard: 0.772114 train_jaccard_postprocessing: 0.970702 train_jaccard_no_postprocessing: 0.630674
validation loss: 0.675913 eval_jaccard: 0.706377 eval_jaccard_postprocessing: 0.971712 eval_jaccard_no_postprocessing: 0.516343
Epoch7

lr: 0.000012 train loss: 0.555693 train_jaccard: 0.779776 train_jaccard_postprocessing: 0.975864 train_jaccard_no_postprocessing: 0.636619
lr: 0.000011 train loss: 0.580882 train_jaccard: 0.779111 train_jaccard_postprocessing: 0.973001 train_jaccard_no_postprocessing: 0.640835
lr: 0.000009 train loss: 0.573591 train_jaccard: 0.777232 train_jaccard_postprocessing: 0.971175 train_jaccard_no_postprocessing: 0.639307
validation loss: 0.682582 eval_jaccard: 0.707341 eval_jaccard_postprocessing: 0.971712 eval_jaccard_no_postprocessing: 0.517997
lr: 0.000008 train loss: 0.569632 train_jaccard: 0.777497 train_jaccard_postprocessing: 0.970615 train_jaccard_no_postprocessing: 0.638864
lr: 0.000007 train loss: 0.584837 train_jaccard: 0.776216 train_jaccard_postprocessing: 0.970608 train_jaccard_no_postprocessing: 0.637972
lr: 0.000005 train loss: 0.569857 train_jaccard: 0.777291 train_jaccard_postprocessing: 0.969656 train_jaccard_no_postprocessing: 0.639844
validation loss: 0.689888 eval_jaccard: 0.708332 eval_jaccard_postprocessing: 0.971712 eval_jaccard_no_postprocessing: 0.519699
lr: 0.000004 train loss: 0.570096 train_jaccard: 0.778148 train_jaccard_postprocessing: 0.970121 train_jaccard_no_postprocessing: 0.641990
lr: 0.000003 train loss: 0.598983 train_jaccard: 0.777269 train_jaccard_postprocessing: 0.970180 train_jaccard_no_postprocessing: 0.640624
lr: 0.000001 train loss: 0.587888 train_jaccard: 0.778038 train_jaccard_postprocessing: 0.970562 train_jaccard_no_postprocessing: 0.640836
lr: 0.000000 train loss: 0.582133 train_jaccard: 0.778820 train_jaccard_postprocessing: 0.970649 train_jaccard_no_postprocessing: 0.642296
validation loss: 0.686662 eval_jaccard: 0.706263 eval_jaccard_postprocessing: 0.971712 eval_jaccard_no_postprocessing: 0.516148
Epoch8

lr: 0.000000 train loss: 0.561132 train_jaccard: 0.786062 train_jaccard_postprocessing: 0.968631 train_jaccard_no_postprocessing: 0.649245
lr: 0.000000 train loss: 0.590086 train_jaccard: 0.781723 train_jaccard_postprocessing: 0.965561 train_jaccard_no_postprocessing: 0.650739
lr: 0.000000 train loss: 0.562105 train_jaccard: 0.783426 train_jaccard_postprocessing: 0.967661 train_jaccard_no_postprocessing: 0.651086
validation loss: 0.686662 eval_jaccard: 0.706263 eval_jaccard_postprocessing: 0.971712 eval_jaccard_no_postprocessing: 0.516148
lr: 0.000000 train loss: 0.571333 train_jaccard: 0.784130 train_jaccard_postprocessing: 0.969685 train_jaccard_no_postprocessing: 0.650112
lr: 0.000000 train loss: 0.556042 train_jaccard: 0.785159 train_jaccard_postprocessing: 0.972087 train_jaccard_no_postprocessing: 0.651019
lr: 0.000000 train loss: 0.586458 train_jaccard: 0.783306 train_jaccard_postprocessing: 0.972111 train_jaccard_no_postprocessing: 0.648446
validation loss: 0.686662 eval_jaccard: 0.706263 eval_jaccard_postprocessing: 0.971712 eval_jaccard_no_postprocessing: 0.516148
lr: 0.000000 train loss: 0.569162 train_jaccard: 0.781171 train_jaccard_postprocessing: 0.972458 train_jaccard_no_postprocessing: 0.644696
lr: 0.000000 train loss: 0.541438 train_jaccard: 0.781759 train_jaccard_postprocessing: 0.972050 train_jaccard_no_postprocessing: 0.646177
lr: 0.000000 train loss: 0.575939 train_jaccard: 0.781203 train_jaccard_postprocessing: 0.970678 train_jaccard_no_postprocessing: 0.646202
lr: 0.000000 train loss: 0.564312 train_jaccard: 0.781501 train_jaccard_postprocessing: 0.970653 train_jaccard_no_postprocessing: 0.646933
validation loss: 0.686662 eval_jaccard: 0.706263 eval_jaccard_postprocessing: 0.971712 eval_jaccard_no_postprocessing: 0.516148
