
Loading data...
*General Setting*
seed: 42
model: TweetBert
trainable parameters:129,374,217
model's state_dict:
device: cuda
use gpu: True
device num: 2
optimizer: AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-08
    initial_lr: 1e-05
    lr: 0.0
    weight_decay: 0.001

Parameter Group 1
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-08
    initial_lr: 0.0002
    lr: 0.0
    weight_decay: 0.001

Parameter Group 2
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-08
    initial_lr: 1e-05
    lr: 0.0
    weight_decay: 0.0

Parameter Group 3
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-08
    initial_lr: 0.0002
    lr: 0.0
    weight_decay: 0.0
)
reuse model: False
adversarial training: False
** start training here! **
   batch_size=16,  accumulation_steps=1
   experiment  = ['training-k-fold-bert-v2.py']
Epoch0

lr: 0.000071 train loss: 2.738691 train_jaccard: 0.539609 train_jaccard_postprocessing: 0.970016 train_jaccard_no_postprocessing: 0.207322
lr: 0.000104 train loss: 1.375227 train_jaccard: 0.573535 train_jaccard_postprocessing: 0.971427 train_jaccard_no_postprocessing: 0.276186
lr: 0.000103 train loss: 1.172521 train_jaccard: 0.596234 train_jaccard_postprocessing: 0.969436 train_jaccard_no_postprocessing: 0.314291
validation loss: 0.820323 eval_jaccard: 0.672407 eval_jaccard_postprocessing: 0.972674 eval_jaccard_no_postprocessing: 0.460103
Validation metric improved (-inf --> 0.672407).  Saving model ...Model saved as /media/jionie/my_disk/Kaggle/Tweet/model/TweetBert/roberta-base-42/fold_3/458_step_0_epoch.pth.lr: 0.000102 train loss: 1.037417 train_jaccard: 0.609996 train_jaccard_postprocessing: 0.969838 train_jaccard_no_postprocessing: 0.346549
lr: 0.000100 train loss: 0.997847 train_jaccard: 0.619565 train_jaccard_postprocessing: 0.968531 train_jaccard_no_postprocessing: 0.369054
lr: 0.000099 train loss: 0.922148 train_jaccard: 0.626674 train_jaccard_postprocessing: 0.969575 train_jaccard_no_postprocessing: 0.381514
validation loss: 0.674099 eval_jaccard: 0.699318 eval_jaccard_postprocessing: 0.972674 eval_jaccard_no_postprocessing: 0.506041
Validation metric improved (0.672407 --> 0.699318).  Saving model ...Model saved as /media/jionie/my_disk/Kaggle/Tweet/model/TweetBert/roberta-base-42/fold_3/916_step_0_epoch.pth.lr: 0.000098 train loss: 0.896494 train_jaccard: 0.633837 train_jaccard_postprocessing: 0.969705 train_jaccard_no_postprocessing: 0.395365
lr: 0.000096 train loss: 0.840445 train_jaccard: 0.641476 train_jaccard_postprocessing: 0.969757 train_jaccard_no_postprocessing: 0.406915
lr: 0.000095 train loss: 0.892187 train_jaccard: 0.646543 train_jaccard_postprocessing: 0.969982 train_jaccard_no_postprocessing: 0.417196
lr: 0.000094 train loss: 0.864575 train_jaccard: 0.651320 train_jaccard_postprocessing: 0.970318 train_jaccard_no_postprocessing: 0.423863
validation loss: 0.661005 eval_jaccard: 0.698656 eval_jaccard_postprocessing: 0.972674 eval_jaccard_no_postprocessing: 0.504911
Epoch1

lr: 0.000092 train loss: 0.765338 train_jaccard: 0.721839 train_jaccard_postprocessing: 0.972684 train_jaccard_no_postprocessing: 0.529612
lr: 0.000091 train loss: 0.819136 train_jaccard: 0.712255 train_jaccard_postprocessing: 0.970954 train_jaccard_no_postprocessing: 0.520184
lr: 0.000090 train loss: 0.845317 train_jaccard: 0.702712 train_jaccard_postprocessing: 0.968642 train_jaccard_no_postprocessing: 0.511212
validation loss: 0.646233 eval_jaccard: 0.706820 eval_jaccard_postprocessing: 0.972674 eval_jaccard_no_postprocessing: 0.518847
Validation metric improved (0.699318 --> 0.706820).  Saving model ...Model saved as /media/jionie/my_disk/Kaggle/Tweet/model/TweetBert/roberta-base-42/fold_3/1832_step_1_epoch.pth.lr: 0.000088 train loss: 0.817351 train_jaccard: 0.703772 train_jaccard_postprocessing: 0.968953 train_jaccard_no_postprocessing: 0.514742
lr: 0.000087 train loss: 0.805098 train_jaccard: 0.705655 train_jaccard_postprocessing: 0.969327 train_jaccard_no_postprocessing: 0.516586
lr: 0.000086 train loss: 0.796393 train_jaccard: 0.705752 train_jaccard_postprocessing: 0.970119 train_jaccard_no_postprocessing: 0.514961
validation loss: 0.629584 eval_jaccard: 0.706910 eval_jaccard_postprocessing: 0.972674 eval_jaccard_no_postprocessing: 0.519000
Validation metric improved (0.706820 --> 0.706910).  Saving model ...Model saved as /media/jionie/my_disk/Kaggle/Tweet/model/TweetBert/roberta-base-42/fold_3/2290_step_1_epoch.pth.lr: 0.000084 train loss: 0.796186 train_jaccard: 0.705867 train_jaccard_postprocessing: 0.970862 train_jaccard_no_postprocessing: 0.513947
lr: 0.000083 train loss: 0.824956 train_jaccard: 0.704388 train_jaccard_postprocessing: 0.970684 train_jaccard_no_postprocessing: 0.513626
lr: 0.000082 train loss: 0.828922 train_jaccard: 0.702907 train_jaccard_postprocessing: 0.970561 train_jaccard_no_postprocessing: 0.513672
lr: 0.000080 train loss: 0.782562 train_jaccard: 0.703850 train_jaccard_postprocessing: 0.970641 train_jaccard_no_postprocessing: 0.513548
validation loss: 0.626752 eval_jaccard: 0.710957 eval_jaccard_postprocessing: 0.972674 eval_jaccard_no_postprocessing: 0.525910
Validation metric improved (0.706910 --> 0.710957).  Saving model ...Model saved as /media/jionie/my_disk/Kaggle/Tweet/model/TweetBert/roberta-base-42/fold_3/2748_step_1_epoch.pth.Epoch2

lr: 0.000079 train loss: 0.730448 train_jaccard: 0.732993 train_jaccard_postprocessing: 0.975156 train_jaccard_no_postprocessing: 0.550499
lr: 0.000078 train loss: 0.748635 train_jaccard: 0.725326 train_jaccard_postprocessing: 0.971277 train_jaccard_no_postprocessing: 0.549756
lr: 0.000076 train loss: 0.779754 train_jaccard: 0.722668 train_jaccard_postprocessing: 0.970243 train_jaccard_no_postprocessing: 0.548904
validation loss: 0.633056 eval_jaccard: 0.707404 eval_jaccard_postprocessing: 0.972674 eval_jaccard_no_postprocessing: 0.519845
lr: 0.000075 train loss: 0.756652 train_jaccard: 0.720557 train_jaccard_postprocessing: 0.969798 train_jaccard_no_postprocessing: 0.544716
lr: 0.000074 train loss: 0.750671 train_jaccard: 0.723154 train_jaccard_postprocessing: 0.969345 train_jaccard_no_postprocessing: 0.549318
lr: 0.000072 train loss: 0.724726 train_jaccard: 0.724048 train_jaccard_postprocessing: 0.970641 train_jaccard_no_postprocessing: 0.547634
validation loss: 0.633939 eval_jaccard: 0.707113 eval_jaccard_postprocessing: 0.972674 eval_jaccard_no_postprocessing: 0.519347
lr: 0.000071 train loss: 0.759166 train_jaccard: 0.724602 train_jaccard_postprocessing: 0.970508 train_jaccard_no_postprocessing: 0.547742
lr: 0.000070 train loss: 0.722925 train_jaccard: 0.724270 train_jaccard_postprocessing: 0.970458 train_jaccard_no_postprocessing: 0.547000
lr: 0.000068 train loss: 0.759714 train_jaccard: 0.723081 train_jaccard_postprocessing: 0.970269 train_jaccard_no_postprocessing: 0.545337
lr: 0.000067 train loss: 0.751779 train_jaccard: 0.722426 train_jaccard_postprocessing: 0.970444 train_jaccard_no_postprocessing: 0.545280
validation loss: 0.622283 eval_jaccard: 0.709900 eval_jaccard_postprocessing: 0.972674 eval_jaccard_no_postprocessing: 0.524105
Epoch3

lr: 0.000066 train loss: 0.695209 train_jaccard: 0.744253 train_jaccard_postprocessing: 0.969232 train_jaccard_no_postprocessing: 0.577217
lr: 0.000064 train loss: 0.682888 train_jaccard: 0.746684 train_jaccard_postprocessing: 0.970231 train_jaccard_no_postprocessing: 0.582713
lr: 0.000063 train loss: 0.737126 train_jaccard: 0.737554 train_jaccard_postprocessing: 0.971315 train_jaccard_no_postprocessing: 0.567423
validation loss: 0.638341 eval_jaccard: 0.711285 eval_jaccard_postprocessing: 0.972674 eval_jaccard_no_postprocessing: 0.526470
Validation metric improved (0.710957 --> 0.711285).  Saving model ...Model saved as /media/jionie/my_disk/Kaggle/Tweet/model/TweetBert/roberta-base-42/fold_3/4580_step_3_epoch.pth.lr: 0.000062 train loss: 0.681805 train_jaccard: 0.736478 train_jaccard_postprocessing: 0.972049 train_jaccard_no_postprocessing: 0.567052
lr: 0.000060 train loss: 0.713630 train_jaccard: 0.736507 train_jaccard_postprocessing: 0.970456 train_jaccard_no_postprocessing: 0.570380
lr: 0.000059 train loss: 0.681360 train_jaccard: 0.735795 train_jaccard_postprocessing: 0.971001 train_jaccard_no_postprocessing: 0.568053
validation loss: 0.630547 eval_jaccard: 0.712237 eval_jaccard_postprocessing: 0.972674 eval_jaccard_no_postprocessing: 0.528094
Validation metric improved (0.711285 --> 0.712237).  Saving model ...Model saved as /media/jionie/my_disk/Kaggle/Tweet/model/TweetBert/roberta-base-42/fold_3/5038_step_3_epoch.pth.lr: 0.000058 train loss: 0.720257 train_jaccard: 0.733573 train_jaccard_postprocessing: 0.970582 train_jaccard_no_postprocessing: 0.564704
lr: 0.000056 train loss: 0.715459 train_jaccard: 0.734514 train_jaccard_postprocessing: 0.970838 train_jaccard_no_postprocessing: 0.566726
lr: 0.000055 train loss: 0.695029 train_jaccard: 0.735626 train_jaccard_postprocessing: 0.970120 train_jaccard_no_postprocessing: 0.569105
lr: 0.000054 train loss: 0.678955 train_jaccard: 0.737305 train_jaccard_postprocessing: 0.970355 train_jaccard_no_postprocessing: 0.570913
validation loss: 0.635118 eval_jaccard: 0.714192 eval_jaccard_postprocessing: 0.972674 eval_jaccard_no_postprocessing: 0.531431
Validation metric improved (0.712237 --> 0.714192).  Saving model ...Model saved as /media/jionie/my_disk/Kaggle/Tweet/model/TweetBert/roberta-base-42/fold_3/5496_step_3_epoch.pth.Epoch4

lr: 0.000052 train loss: 0.672510 train_jaccard: 0.750589 train_jaccard_postprocessing: 0.969052 train_jaccard_no_postprocessing: 0.594934
lr: 0.000051 train loss: 0.682371 train_jaccard: 0.745148 train_jaccard_postprocessing: 0.967687 train_jaccard_no_postprocessing: 0.591874
lr: 0.000049 train loss: 0.649979 train_jaccard: 0.749269 train_jaccard_postprocessing: 0.968863 train_jaccard_no_postprocessing: 0.596973
validation loss: 0.652035 eval_jaccard: 0.709827 eval_jaccard_postprocessing: 0.972674 eval_jaccard_no_postprocessing: 0.523981
lr: 0.000048 train loss: 0.625597 train_jaccard: 0.752664 train_jaccard_postprocessing: 0.970778 train_jaccard_no_postprocessing: 0.597986
lr: 0.000047 train loss: 0.662581 train_jaccard: 0.750590 train_jaccard_postprocessing: 0.970119 train_jaccard_no_postprocessing: 0.593470
lr: 0.000045 train loss: 0.645092 train_jaccard: 0.752064 train_jaccard_postprocessing: 0.969757 train_jaccard_no_postprocessing: 0.597104
validation loss: 0.639904 eval_jaccard: 0.712639 eval_jaccard_postprocessing: 0.972674 eval_jaccard_no_postprocessing: 0.528780
lr: 0.000044 train loss: 0.677813 train_jaccard: 0.751025 train_jaccard_postprocessing: 0.969856 train_jaccard_no_postprocessing: 0.595901
lr: 0.000043 train loss: 0.654706 train_jaccard: 0.751051 train_jaccard_postprocessing: 0.970239 train_jaccard_no_postprocessing: 0.594587
lr: 0.000041 train loss: 0.652028 train_jaccard: 0.751133 train_jaccard_postprocessing: 0.970601 train_jaccard_no_postprocessing: 0.593519
lr: 0.000040 train loss: 0.669335 train_jaccard: 0.750412 train_jaccard_postprocessing: 0.970508 train_jaccard_no_postprocessing: 0.593299
validation loss: 0.645043 eval_jaccard: 0.715515 eval_jaccard_postprocessing: 0.972674 eval_jaccard_no_postprocessing: 0.533689
Validation metric improved (0.714192 --> 0.715515).  Saving model ...Model saved as /media/jionie/my_disk/Kaggle/Tweet/model/TweetBert/roberta-base-42/fold_3/6870_step_4_epoch.pth.Epoch5

lr: 0.000039 train loss: 0.621425 train_jaccard: 0.766132 train_jaccard_postprocessing: 0.972835 train_jaccard_no_postprocessing: 0.625104
lr: 0.000037 train loss: 0.615545 train_jaccard: 0.763316 train_jaccard_postprocessing: 0.971716 train_jaccard_no_postprocessing: 0.617734
lr: 0.000036 train loss: 0.640402 train_jaccard: 0.764157 train_jaccard_postprocessing: 0.971301 train_jaccard_no_postprocessing: 0.620405
validation loss: 0.667121 eval_jaccard: 0.709709 eval_jaccard_postprocessing: 0.972674 eval_jaccard_no_postprocessing: 0.523779
lr: 0.000035 train loss: 0.599968 train_jaccard: 0.766100 train_jaccard_postprocessing: 0.970729 train_jaccard_no_postprocessing: 0.622546
lr: 0.000033 train loss: 0.600922 train_jaccard: 0.766843 train_jaccard_postprocessing: 0.971990 train_jaccard_no_postprocessing: 0.621279
lr: 0.000032 train loss: 0.630398 train_jaccard: 0.764356 train_jaccard_postprocessing: 0.971914 train_jaccard_no_postprocessing: 0.617485
validation loss: 0.662749 eval_jaccard: 0.712080 eval_jaccard_postprocessing: 0.972674 eval_jaccard_no_postprocessing: 0.527827
lr: 0.000031 train loss: 0.602410 train_jaccard: 0.765799 train_jaccard_postprocessing: 0.970939 train_jaccard_no_postprocessing: 0.619402
lr: 0.000029 train loss: 0.652253 train_jaccard: 0.764561 train_jaccard_postprocessing: 0.970569 train_jaccard_no_postprocessing: 0.617642
lr: 0.000028 train loss: 0.602866 train_jaccard: 0.765018 train_jaccard_postprocessing: 0.970440 train_jaccard_no_postprocessing: 0.618257
lr: 0.000027 train loss: 0.638632 train_jaccard: 0.763953 train_jaccard_postprocessing: 0.970330 train_jaccard_no_postprocessing: 0.616523
validation loss: 0.654435 eval_jaccard: 0.712118 eval_jaccard_postprocessing: 0.972674 eval_jaccard_no_postprocessing: 0.527891
Epoch6

lr: 0.000025 train loss: 0.593782 train_jaccard: 0.778534 train_jaccard_postprocessing: 0.969559 train_jaccard_no_postprocessing: 0.644208
lr: 0.000024 train loss: 0.594570 train_jaccard: 0.779816 train_jaccard_postprocessing: 0.969698 train_jaccard_no_postprocessing: 0.645916
lr: 0.000023 train loss: 0.590511 train_jaccard: 0.777726 train_jaccard_postprocessing: 0.969848 train_jaccard_no_postprocessing: 0.641608
validation loss: 0.674520 eval_jaccard: 0.711844 eval_jaccard_postprocessing: 0.972674 eval_jaccard_no_postprocessing: 0.527424
lr: 0.000021 train loss: 0.591349 train_jaccard: 0.775770 train_jaccard_postprocessing: 0.969658 train_jaccard_no_postprocessing: 0.637820
lr: 0.000020 train loss: 0.609847 train_jaccard: 0.773985 train_jaccard_postprocessing: 0.970303 train_jaccard_no_postprocessing: 0.635418
lr: 0.000019 train loss: 0.580804 train_jaccard: 0.773410 train_jaccard_postprocessing: 0.970643 train_jaccard_no_postprocessing: 0.632838
validation loss: 0.677442 eval_jaccard: 0.712539 eval_jaccard_postprocessing: 0.972674 eval_jaccard_no_postprocessing: 0.528610
lr: 0.000017 train loss: 0.587430 train_jaccard: 0.774114 train_jaccard_postprocessing: 0.970809 train_jaccard_no_postprocessing: 0.635242
lr: 0.000016 train loss: 0.618328 train_jaccard: 0.772466 train_jaccard_postprocessing: 0.970678 train_jaccard_no_postprocessing: 0.632330
lr: 0.000015 train loss: 0.602198 train_jaccard: 0.773328 train_jaccard_postprocessing: 0.970589 train_jaccard_no_postprocessing: 0.632691
lr: 0.000013 train loss: 0.596766 train_jaccard: 0.772871 train_jaccard_postprocessing: 0.970405 train_jaccard_no_postprocessing: 0.631863
validation loss: 0.669809 eval_jaccard: 0.710145 eval_jaccard_postprocessing: 0.972674 eval_jaccard_no_postprocessing: 0.524522
Epoch7

lr: 0.000012 train loss: 0.579069 train_jaccard: 0.782011 train_jaccard_postprocessing: 0.965337 train_jaccard_no_postprocessing: 0.650900
lr: 0.000011 train loss: 0.539592 train_jaccard: 0.784856 train_jaccard_postprocessing: 0.968507 train_jaccard_no_postprocessing: 0.651153
lr: 0.000009 train loss: 0.556518 train_jaccard: 0.783276 train_jaccard_postprocessing: 0.969360 train_jaccard_no_postprocessing: 0.646060
validation loss: 0.681742 eval_jaccard: 0.711367 eval_jaccard_postprocessing: 0.972674 eval_jaccard_no_postprocessing: 0.526609
lr: 0.000008 train loss: 0.582036 train_jaccard: 0.782314 train_jaccard_postprocessing: 0.968622 train_jaccard_no_postprocessing: 0.646677
lr: 0.000007 train loss: 0.573161 train_jaccard: 0.781526 train_jaccard_postprocessing: 0.969716 train_jaccard_no_postprocessing: 0.642999
lr: 0.000005 train loss: 0.584615 train_jaccard: 0.780703 train_jaccard_postprocessing: 0.969103 train_jaccard_no_postprocessing: 0.643072
validation loss: 0.678251 eval_jaccard: 0.711559 eval_jaccard_postprocessing: 0.972674 eval_jaccard_no_postprocessing: 0.526937
lr: 0.000004 train loss: 0.579526 train_jaccard: 0.781330 train_jaccard_postprocessing: 0.969473 train_jaccard_no_postprocessing: 0.644632
lr: 0.000003 train loss: 0.566480 train_jaccard: 0.780736 train_jaccard_postprocessing: 0.970264 train_jaccard_no_postprocessing: 0.644743
lr: 0.000001 train loss: 0.587503 train_jaccard: 0.779725 train_jaccard_postprocessing: 0.969983 train_jaccard_no_postprocessing: 0.643629
lr: 0.000000 train loss: 0.573122 train_jaccard: 0.780152 train_jaccard_postprocessing: 0.970499 train_jaccard_no_postprocessing: 0.644352
validation loss: 0.682965 eval_jaccard: 0.711619 eval_jaccard_postprocessing: 0.972674 eval_jaccard_no_postprocessing: 0.527039
