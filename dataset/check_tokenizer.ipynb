{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0511 12:35:47.221462 139756695570176 file_utils.py:41] PyTorch version 1.4.0 available.\n",
      "I0511 12:35:48.351346 139756695570176 file_utils.py:57] TensorFlow version 2.1.0 available.\n"
     ]
    }
   ],
   "source": [
    "from transformers import *\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0512 00:00:30.669103 139756695570176 tokenization_utils.py:420] Model name 'transformers_vocab/albert-large-v2-spiece.model' not found in model shortcut name list (albert-base-v1, albert-large-v1, albert-xlarge-v1, albert-xxlarge-v1, albert-base-v2, albert-large-v2, albert-xlarge-v2, albert-xxlarge-v2). Assuming 'transformers_vocab/albert-large-v2-spiece.model' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "W0512 00:00:30.670040 139756695570176 tokenization_utils.py:432] Calling AlbertTokenizer.from_pretrained() with the path to a single file or url is deprecated\n",
      "I0512 00:00:30.670568 139756695570176 tokenization_utils.py:502] loading file transformers_vocab/albert-large-v2-spiece.model\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AlbertTokenizer.from_pretrained(\n",
    "            pretrained_model_name_or_path=\"transformers_vocab/albert-large-v2-spiece.model\",\n",
    "            lowercase=True,\n",
    "            add_prefix_space=True\n",
    "        )\n",
    "\n",
    "def get_prediction(tweet, selected_text):\n",
    "    \n",
    "    tweet = \" \".join(tweet.split())\n",
    "    selected_text = \" \".join(selected_text.split())\n",
    "    \n",
    "    selected_text_copy = \"\".join(selected_text.split())\n",
    "    tweet_copy = \"\".join(tweet.split())\n",
    "\n",
    "    len_st = len(selected_text_copy)\n",
    "    idx0 = None\n",
    "    idx1 = None\n",
    "\n",
    "    for ind in (i for i, e in enumerate(tweet_copy) if e == selected_text_copy[0]):\n",
    "        if tweet_copy[ind: ind + len_st] == selected_text_copy:\n",
    "            idx0 = ind\n",
    "            idx1 = ind + len_st - 1\n",
    "            break\n",
    "    \n",
    "    char_targets = [0] * len(tweet_copy)\n",
    "    if idx0 != None and idx1 != None:\n",
    "        for ct in range(idx0, idx1 + 1):\n",
    "            char_targets[ct] = 1\n",
    "            \n",
    "    input_ids_orig = tokenizer.encode(tweet)\n",
    "    input_ids_orig = input_ids_orig[1:-1]\n",
    "    \n",
    "    tweet_offsets = []\n",
    "    idx = 0\n",
    "    for t in input_ids_orig:\n",
    "        w = tokenizer.decode([t])\n",
    "        tweet_offsets.append((idx, idx + len(w)))\n",
    "        idx += len(w)\n",
    "        \n",
    "    target_idx = []\n",
    "    for j, (offset1, offset2) in enumerate(tweet_offsets):\n",
    "        if sum(char_targets[offset1: offset2]) > 0:\n",
    "            target_idx.append(j)\n",
    "            \n",
    "    targets_start = target_idx[0]\n",
    "    targets_end = target_idx[-1]\n",
    "    tweet_copy = \"\".join(tweet.split())\n",
    "\n",
    "    # ... i kinda lost respect  . i kinda lost respect\n",
    "    prediction = \"\"\n",
    "    for ix in range(targets_start, targets_end + 1):\n",
    "        prediction += tweet_copy[tweet_offsets[ix][0]: tweet_offsets[ix][1]]\n",
    "        if (ix + 1) < len(tweet_offsets) and tweet_offsets[ix][1] < tweet_offsets[ix + 1][0]:\n",
    "            prediction += \" \"\n",
    "            \n",
    "    len_prediction = len(prediction)\n",
    "\n",
    "    for ind in (i for i, e in enumerate(tweet) if e == prediction[0]):\n",
    "\n",
    "        tweet_sub_sentence = \"\".join(tweet[ind:].split())\n",
    "\n",
    "        if tweet_sub_sentence[:len_prediction] == prediction:\n",
    "            orig_idx0 = ind\n",
    "            break\n",
    "\n",
    "    for ind in range(orig_idx0, len(tweet)):\n",
    "\n",
    "        if \"\".join(tweet[orig_idx0: ind+1].split()) == prediction:\n",
    "            filtered_output = tweet[orig_idx0: ind+1]\n",
    "            break\n",
    "            \n",
    "    return filtered_output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0512 18:29:05.413665 139756695570176 tokenization_utils.py:420] Model name 'transformers_vocab/albert-large-v2-spiece.model' not found in model shortcut name list (albert-base-v1, albert-large-v1, albert-xlarge-v1, albert-xxlarge-v1, albert-base-v2, albert-large-v2, albert-xlarge-v2, albert-xxlarge-v2). Assuming 'transformers_vocab/albert-large-v2-spiece.model' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "W0512 18:29:05.414740 139756695570176 tokenization_utils.py:432] Calling AlbertTokenizer.from_pretrained() with the path to a single file or url is deprecated\n",
      "I0512 18:29:05.415167 139756695570176 tokenization_utils.py:502] loading file transformers_vocab/albert-large-v2-spiece.model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['*sigh*']\n",
      " *sigh*\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AlbertTokenizer.from_pretrained(\n",
    "            pretrained_model_name_or_path=\"transformers_vocab/albert-large-v2-spiece.model\",\n",
    "            lowercase=True,\n",
    "            add_prefix_space=True\n",
    "        )\n",
    "\n",
    "def get_prediction_v2(tweet, selected_text):\n",
    "    \n",
    "    tweet = \" \".join(tweet.split())\n",
    "    selected_text = \" \".join(selected_text.split())\n",
    "    \n",
    "    len_st = len(selected_text)\n",
    "    idx0 = None\n",
    "    idx1 = None\n",
    "    \n",
    "    # get char idx\n",
    "    for ind in (i for i, e in enumerate(tweet) if e == selected_text[0]):\n",
    "        if tweet[ind: ind + len_st] == selected_text:\n",
    "            idx0 = ind\n",
    "            idx1 = ind + len_st - 1\n",
    "            break\n",
    "    \n",
    "    # get char mask\n",
    "    char_targets = [0] * len(tweet)\n",
    "    if idx0 != None and idx1 != None:\n",
    "        for ct in range(idx0, idx1 + 1):\n",
    "            char_targets[ct] = 1\n",
    "    \n",
    "    # get word offsets\n",
    "    tweet_offsets = []\n",
    "    idx = 0\n",
    "    \n",
    "    for word in tweet.split():\n",
    "        tweet_offsets.append((idx, idx + len(word)))\n",
    "        # add \" \", we have done \" \".join() before so only one \" \" between words\n",
    "        idx += len(word) + 1\n",
    "    \n",
    "    # get word idx\n",
    "    target_idx = []\n",
    "    for j, (offset1, offset2) in enumerate(tweet_offsets):\n",
    "    \n",
    "        if sum(char_targets[offset1: offset2]) > 0:\n",
    "            target_idx.append(j)\n",
    "            \n",
    "    targets_start = target_idx[0]\n",
    "    targets_end = target_idx[-1]\n",
    "    \n",
    "#     print(tweet.split()[targets_start: targets_end+1])\n",
    "    \n",
    "    # get sub_token idx\n",
    "    tweet_word = tweet.split()\n",
    "    selected_text_word = selected_text.split()\n",
    "    word_to_first_token_index = {}\n",
    "    word_to_last_token_index = {}\n",
    "    token_to_word_index = {}\n",
    "    sub_token_idx = 0\n",
    "    \n",
    "    for (i, word) in enumerate(tweet_word):\n",
    "        \n",
    "        sub_tokens = tokenizer.tokenize(word)\n",
    "        \n",
    "        word_to_first_token_index[i] = sub_token_idx\n",
    "        \n",
    "        for sub_token in sub_tokens:\n",
    "            \n",
    "            token_to_word_index[sub_token_idx] = i\n",
    "            sub_token_idx += 1\n",
    "            \n",
    "        word_to_last_token_index[i] = sub_token_idx - 1\n",
    "        \n",
    "    targets_start_token = word_to_first_token_index[targets_start]\n",
    "    targets_end_token = word_to_first_token_index[targets_end]\n",
    "            \n",
    "    encoded = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(tweet))\n",
    "    \n",
    "#     return tweet[token_to_word_index[targets_start_token]: token_to_word_index[targets_end_token]]\n",
    "    \n",
    "    print(tweet[token_to_word_index[targets_start_token]: token_to_word_index[targets_end_token]], selected_text)\n",
    "\n",
    "tweet = \"S`ok, trying to plot alternatives as we speak *sigh*\"\n",
    "selected_text = \"*sigh*\"\n",
    "get_prediction_v2(tweet, selected_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0512 19:37:48.308559 139756695570176 tokenization_utils.py:420] Model name 'transformers_vocab/albert-large-v2-spiece.model' not found in model shortcut name list (albert-base-v1, albert-large-v1, albert-xlarge-v1, albert-xxlarge-v1, albert-base-v2, albert-large-v2, albert-xlarge-v2, albert-xxlarge-v2). Assuming 'transformers_vocab/albert-large-v2-spiece.model' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "W0512 19:37:48.309523 139756695570176 tokenization_utils.py:432] Calling AlbertTokenizer.from_pretrained() with the path to a single file or url is deprecated\n",
      "I0512 19:37:48.309928 139756695570176 tokenization_utils.py:502] loading file transformers_vocab/albert-large-v2-spiece.model\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AlbertTokenizer.from_pretrained(\n",
    "            pretrained_model_name_or_path=\"transformers_vocab/albert-large-v2-spiece.model\",\n",
    "            lowercase=True,\n",
    "            add_prefix_space=True\n",
    "        )\n",
    "\n",
    "def get_prediction_v3(tweet, selected_text):\n",
    "    \n",
    "    tweet = \" \".join(tweet.split())\n",
    "    selected_text = \" \".join(selected_text.split())\n",
    "    \n",
    "    len_st = len(selected_text)\n",
    "    idx0 = None\n",
    "    idx1 = None\n",
    "    \n",
    "    # get char idx\n",
    "    for ind in (i for i, e in enumerate(tweet) if e == selected_text[0]):\n",
    "        if tweet[ind: ind + len_st] == selected_text:\n",
    "            idx0 = ind\n",
    "            idx1 = ind + len_st - 1\n",
    "            break\n",
    "    \n",
    "    # get char mask\n",
    "    char_targets = [0] * len(tweet)\n",
    "    if idx0 != None and idx1 != None:\n",
    "        for ct in range(idx0, idx1 + 1):\n",
    "            char_targets[ct] = 1\n",
    "    \n",
    "    # get word offsets\n",
    "    tweet_offsets = []\n",
    "    cursor = 0\n",
    "    \n",
    "    for word in tweet.split():\n",
    "\n",
    "        encoded_word = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(word))\n",
    "        number_of_tokens = len(encoded_word)\n",
    "        \n",
    "        start_offsets = cursor\n",
    "        cursor += len(word)\n",
    "        end_offsets = cursor\n",
    "        \n",
    "        for i in range(number_of_tokens):\n",
    "            tweet_offsets.append((start_offsets, end_offsets))\n",
    "            \n",
    "        cursor += 1\n",
    "    \n",
    "    # get word idx\n",
    "    target_idx = []\n",
    "    for j, (offset1, offset2) in enumerate(tweet_offsets):\n",
    "        \n",
    "        if sum(char_targets[offset1: offset2]) > 0:\n",
    "            target_idx.append(j)\n",
    "    \n",
    "    targets_start = target_idx[0]\n",
    "    targets_end = target_idx[-1]\n",
    "    \n",
    "            \n",
    "    input_ids_orig = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(tweet))\n",
    "    \n",
    "    return tweet[tweet_offsets[targets_start][0]: tweet_offsets[targets_end][1]]\n",
    "\n",
    "# tweet = \"S`ok, trying to plot alternatives as we speak *sigh*\"\n",
    "# selected_text = \"*sigh*\"\n",
    "# get_prediction_v3(tweet, selected_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"/media/jionie/my_disk/Kaggle/Tweet/input/tweet-sentiment-extraction/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I`d have responded, if I were going ------------------- I`d have responded, if I were going\n",
      "Sooo SAD ------------------- Sooo SAD\n",
      "bullying me ------------------- bullying me...\n",
      "leave me alone ------------------- leave me alone\n",
      "Sons of ****, ------------------- Sons of ****,\n",
      "http://www.dothebouncy.com/smf - some shameless plugging for the best Rangers forum on earth ------------------- http://www.dothebouncy.com/smf - some shameless plugging for the best Rangers forum on earth\n",
      "fun ------------------- fun\n",
      "Soooo high ------------------- Soooo high\n",
      "Both of you ------------------- Both of you\n",
      "Wow... u just became cooler. ------------------- Wow... u just became cooler.\n",
      "as much as i love to be hopeful, i reckon the chances are minimal =P i`m never gonna get my cake and stuff ------------------- as much as i love to be hopeful, i reckon the chances are minimal =P i`m never gonna get my cake and stuff\n",
      "like ------------------- like\n",
      "DANGERously ------------------- DANGERously\n",
      "lost ------------------- lost\n",
      "test test from the LG enV2 ------------------- test test from the LG enV2\n",
      "Uh oh, I am sunburned ------------------- Uh oh, I am sunburned\n",
      "*sigh* ------------------- *sigh*\n",
      "sick ------------------- sick\n",
      "onna ------------------- gonna\n",
      "Hes just not that into you ------------------- Hes just not that into you\n",
      "oh Marly, I`m so sorry!!  I hope you find her soon!! <3 <3 ------------------- oh Marly, I`m so sorry!! I hope you find her soon!! <3 <3\n",
      "interesting. ------------------- interesting.\n",
      "is cleaning the house for her family who is comming later today.. ------------------- is cleaning the house for her family who is comming later today..\n",
      "gotta restart my computer .. I thought Win7 was supposed to put an end to the constant rebootiness ------------------- gotta restart my computer .. I thought Win7 was supposed to put an end to the constant rebootiness\n",
      "SEe waT I Mean bOuT FoLL0w fRiiDaYs... It`S cALLed LoSe f0LloWeRs FridAy... smH ------------------- SEe waT I Mean bOuT FoLL0w fRiiDaYs... It`S cALLed LoSe f0LloWeRs FridAy... smH\n",
      "the free fillin` app on my ipod is fun, im addicted ------------------- the free fillin` app on my ipod is fun, im addicted\n",
      "I`m sorry. ------------------- I`m sorry.\n",
      ".no internet ------------------- Malaysia...no internet\n",
      "fun ------------------- fun\n",
      "Power back up not working too ------------------- Power back up not working too\n",
      "Quite....heavenly ------------------- Quite....heavenly\n",
      "hope ------------------- hope\n",
      "well so much for being unhappy for about 10 minute ------------------- well so much for being unhappy for about 10 minutes.\n",
      "funny. ------------------- funny.\n",
      "Ahhh, I slept through the game.  I`m gonna try my best to watch tomorrow though. I hope we play Army. ------------------- Ahhh, I slept through the game. I`m gonna try my best to watch tomorrow though. I hope we play Army.\n",
      "Thats it, its the end. Tears for Fears ------------------- Thats it, its the end. Tears for Fears\n",
      "miss ------------------- miss\n",
      "just in case you wonder, we are really busy today and this coming with with adding tons of new blogs and updates stay tuned ------------------- just in case you wonder, we are really busy today and this coming with with adding tons of new blogs and updates stay tuned\n",
      "soooooo sleeeeepy!!! ------------------- soooooo sleeeeepy!!!\n",
      "A little happy fo ------------------- A little happy for\n",
      "Car not happy, big big dent in boot! Hoping theyre not going to write it off, crossing fingers and waiting ------------------- Car not happy, big big dent in boot! Hoping theyre not going to write it off, crossing fingers and waiting\n",
      "avid fan of ------------------- avid fan of\n",
      "MAYDAY?! ------------------- MAYDAY?!\n",
      "RATT ROCKED NASHVILLE TONITE..ONE THING SUCKED, NO ENCORE!  LIKE IN THE 80`S THEY STILL HAVE A FUN SHOW. PEARCY HAS THAT HOTT BAD BOY LOOK ------------------- RATT ROCKED NASHVILLE TONITE..ONE THING SUCKED, NO ENCORE! LIKE IN THE 80`S THEY STILL HAVE A FUN SHOW. PEARCY HAS THAT HOTT BAD BOY LOOK\n",
      "I love to! ------------------- I love to!\n",
      "The girl in the hair salon asked me 'Shall I trim your eyebrows!' How old do I feel? ------------------- The girl in the hair salon asked me 'Shall I trim your eyebrows!' How old do I feel?\n",
      "SUCKKKKKK ------------------- SUCKKKKKK\n",
      ":visiting my friendster and facebook ------------------- :visiting my friendster and facebook\n",
      "dont like go ------------------- dont like going\n",
      "d I`m not thrilled at all with mine. ------------------- and I`m not thrilled at all with mine.\n",
      "Then you should check out http://twittersucks.com and connect with other tweeple who hate twitter ------------------- Then you should check out http://twittersucks.com and connect with other tweeple who hate twitter\n",
      "also bored at school, its my third freelesson( freistunde ) ------------------- also bored at school, its my third freelesson( freistunde )\n",
      "hm... Both of us I guess... ------------------- hm... Both of us I guess...\n",
      "it is ****...u have dissappointed me that past few days ------------------- it is ****...u have dissappointed me that past few days\n",
      "romance zero is funny ------------------- romance zero is funny\n",
      "I`d rather do the early run..but I am a morning runner ------------------- I`d rather do the early run..but I am a morning runner\n",
      "hurts ------------------- hurts\n",
      "will be back later. ------------------- will be back later.\n",
      "Torn ace of hearts ------------------- Torn ace of hearts\n",
      "what fun are you speaking of? ------------------- what fun are you speaking of?\n",
      "i lost all my friends, i`m alone and sleepy.. ------------------- i lost all my friends, i`m alone and sleepy..i\n",
      "haha yes ------------------- haha yes\n",
      "I give in to easily ------------------- I give in to easily\n",
      "favorite ------------------- favorite\n",
      "jealous.. ------------------- jealous....\n",
      "Is at a photoshoot. ------------------- Is at a photoshoot.\n",
      "s awesome ------------------- He`s awesome...\n",
      "Yay playing a show tonight! Boo it`s gonna soggy and I`m at work right before playing ------------------- Yay playing a show tonight! Boo it`s gonna soggy and I`m at work right before playing\n",
      "Chilliin ------------------- Chilliin\n",
      "If you know such agent, do let me know ------------------- If you know such agent, do let me know\n",
      "I still smell of smoke  #kitchenfire ------------------- I still smell of smoke #kitchenfire\n",
      "better ------------------- better\n",
      "Anyone have an extra Keane ticket? I promise to buy you a drink and take rad pics for your FB / Blog / Flickr., etc ------------------- Anyone have an extra Keane ticket? I promise to buy you a drink and take rad pics for your FB / Blog / Flickr., etc\n",
      "'you can ride one, you can catch one, but its not summer til you pop open one'  ? ------------------- 'you can ride one, you can catch one, but its not summer til you pop open one' ?\n",
      "she is good! so gor-juz yea i kno i asked her yesterday when we were at tha hospital if she talked to u and she said no ------------------- she is good! so gor-juz yea i kno i asked her yesterday when we were at tha hospital if she talked to u and she said no\n",
      "OK - I`m out of here for now. Just popped in to say Hi and check on things. I`ll probably head to the guttah later on tonight ------------------- OK - I`m out of here for now. Just popped in to say Hi and check on things. I`ll probably head to the guttah later on tonight\n",
      "BADDD. ------------------- BADDD.\n",
      "My sources say no ------------------- My sources say no\n",
      "I am sooo tired ------------------- I am sooo tired\n",
      "Hey, you change your twitter account, and you didn`t even tell me... ------------------- Hey, you change your twitter account, and you didn`t even tell me...\n",
      "THANK YYYYYYYYYOOOOOOOOOOUUUUU! ------------------- THANK YYYYYYYYYOOOOOOOOOOUUUUU!\n",
      "lucky ------------------- lucky\n",
      "fell asleep waiting for my ride! ------------------- fell asleep waiting for my ride!\n",
      "Sick. ------------------- Sick.\n",
      ", sorry guys ------------------- response, sorry guys\n",
      "Happy Star Wars day everyone! and Enjoy the holiday (UK) ------------------- Happy Star Wars day everyone! and Enjoy the holiday (UK)\n",
      "Miles from you   I`m in Essex so give me plenty of warning so I can arrive in time to get at least one of those free beers ------------------- Miles from you I`m in Essex so give me plenty of warning so I can arrive in time to get at least one of those free beers.\n",
      "His snoring is so annoying n it keeps me from sleeping (like right now, lol) but I honestly wud miss it if it eva left  I love him. ------------------- His snoring is so annoying n it keeps me from sleeping (like right now, lol) but I honestly wud miss it if it eva left I love him.\n",
      "i miss you bby ------------------- i miss you bby\n",
      "cool ------------------- cool\n",
      "Love it there ------------------- Love it there\n",
      "_Mounce yes and it lasts way past my bedtime! ------------------- _Mounce yes and it lasts way past my bedtime!\n",
      "Hi  how are you doing ???  *just joined twitter... ------------------- Hi how are you doing ??? *just joined twitter...*\n",
      "tired ------------------- tired\n",
      "eating ice cream and then getting ready for graduation. ------------------- eating ice cream and then getting ready for graduation.\n",
      "Happy Mothers day to all you Mums out there ------------------- Happy Mothers day to all you Mums out there\n",
      "freaked ------------------- freaked\n",
      "unfortunately ------------------- unfortunately\n",
      "Gonna read a story bout adam lambert online then bed. Nighty night ------------------- Gonna read a story bout adam lambert online then bed. Nighty night\n",
      "best ------------------- best\n",
      "Pretty ------------------- Pretty\n",
      "Certainly not Cheers than, huh? ------------------- Certainly not Cheers than, huh?\n",
      "horrible, ------------------- my'horrible,\n",
      "busy ------------------- busy\n",
      "Awesome. ------------------- Awesome.\n",
      "at least I get to watch over time  Let`s go Pens!! ------------------- at least I get to watch over time Let`s go Pens!!\n",
      "cool i wear black most of the time when i go out ------------------- cool i wear black most of the time when i go out\n",
      "thanks ------------------- thanks\n",
      "safe ------------------- safe\n",
      "I wish I was allowed to go ------------------- I wish I was allowed to go\n",
      "if u have a friendster add me!!!!!!!!!        my email adress      add me  loco_crime_1st.com        add me ------------------- if u have a friendster add me!!!!!!!!! my email adress add me loco_crime_1st.com add me\n",
      "has tickets.......? ------------------- has tickets.......?\n",
      "Thank you, ------------------- Thank you,\n",
      "ACSM. it`s unfathomable.  i think the other one .. and the .. is one that should be kept to the comfort of our bedrooms. yes? ------------------- ACSM. it`s unfathomable. i think the other one .. and the .. is one that should be kept to the comfort of our bedrooms. yes?\n",
      "I love my ------------------- I love my\n",
      "I don`t feel confident ------------------- I don`t feel confident\n",
      "sad. ------------------- sad..\n",
      "hahaa your awesomee ! ------------------- hahaa your awesomee !\n",
      "awesomeeeee ------------------- awesomeeeee\n",
      "I hate Fallout 3 it keeps making me jump, I`m also low on health, money, ammo and food  don`t worry I`ll get through it. ------------------- I hate Fallout 3 it keeps making me jump, I`m also low on health, money, ammo and food don`t worry I`ll get through it.\n",
      "I had it! On my itunes, but then I lost all my songs. ------------------- I had it! On my itunes, but then I lost all my songs.\n",
      "What`s with the gloomy weather? The sun must be too tired to come out and play  heading to victoria gardens for some impulse buys haha ------------------- What`s with the gloomy weather? The sun must be too tired to come out and play heading to victoria gardens for some impulse buys haha\n",
      "Not looking forward ------------------- Not looking forward\n",
      "Poor you ------------------- Poor you\n",
      "not well ------------------- not well\n",
      "Not a prob ------------------- Not a prob\n",
      "at dads, watching some mtv and am going on sims2 in a minutee ------------------- at dads, watching some mtv and am going on sims2 in a minutee\n",
      "Absolutely ------------------- Absolutely\n",
      "what`s the matter chickadee? ------------------- what`s the matter chickadee?\n",
      "y adore ------------------- totally adore\n",
      "good ------------------- good\n",
      "I love him too ------------------- I love him too\n",
      "painful. ------------------- painful...but\n",
      "sad? ------------------- sad?\n",
      "e nice ------------------- be nice\n",
      "i wish ------------------- i wish\n",
      "Namaskar & Namaste r both the same. Marathi people say Namaskar! its a marathi word.... should i ? ...naaaah ! ------------------- Namaskar & Namaste r both the same. Marathi people say Namaskar! its a marathi word.... should i ? ...naaaah !\n",
      "Congrats!  I cuss like that in a matter of minutes, But didn`t know until now there is a reward for it. ------------------- Congrats! I cuss like that in a matter of minutes, But didn`t know until now there is a reward for it.\n",
      "Humous and Dorito`s.... Oh yes ------------------- Humous and Dorito`s.... Oh yes\n",
      "missed all the awesome weather, ------------------- missed all the awesome weather,\n",
      "Today is going to be a normal day for I hope. We had a group of pilots from a large airline come in last night so it was too much drink ------------------- Today is going to be a normal day for I hope. We had a group of pilots from a large airline come in last night so it was too much drink\n",
      "terrible! ------------------- terrible!\n",
      "Unfortunatley, ------------------- Unfortunatley,\n",
      "That sucks, tho. ------------------- That sucks, tho.\n",
      "Hate fighting ------------------- Hate fighting\n",
      "I watched that too!!! I didnt want her to win, but she put up a good fight..lol ------------------- I watched that too!!! I didnt want her to win, but she put up a good fight..lol\n",
      "Car-warmed Sprite tastes like sore throat ------------------- Car-warmed Sprite tastes like sore throat\n",
      "Just came 11th in cross country and beat dumbo ------------------- Just came 11th in cross country and beat dumbo\n",
      "enjoyable. ------------------- enjoyable.\n",
      "endearing- ------------------- endearing--because\n",
      "tomorrow valeria`s lunch!!! going to get my hair done but im arraving late   got my cousins babtizm or whatever you spell it ------------------- tomorrow valeria`s lunch!!! going to get my hair done but im arraving late got my cousins babtizm or whatever you spell it\n",
      "goooooddd morning tweets!! ------------------- goooooddd morning tweets!!\n",
      "hate ------------------- hate\n",
      "fine! ------------------- fine!\n",
      "i don`t like the other ones. ------------------- i don`t like the other ones.\n",
      "Mmmmmmmm... ? it in the morning ------------------- Mmmmmmmm... ? it in the morning\n",
      "me neither ------------------- me neither\n",
      "Has about 10 hours work to do, on a Sunday. Boo. I will find time for a two hour lunchbreak though. Yeah ------------------- Has about 10 hours work to do, on a Sunday. Boo. I will find time for a two hour lunchbreak though. Yeah\n",
      "Bugger. forgot I still have washing in my machine ------------------- Bugger. forgot I still have washing in my machine\n",
      "sending love, blessings & healing thoughts to you & family  peace ------------------- sending love, blessings & healing thoughts to you & family peace\n",
      ".really bad ------------------- hurts...really bad\n",
      "ah yes, I know that feeling ------------------- ah yes, I know that feeling\n",
      "Night of the cookers with my dad ------------------- Night of the cookers with my dad\n",
      "brutal ------------------- brutal\n",
      "Nope I am in Coquitlam ------------------- Nope I am in Coquitlam\n",
      "boring ------------------- boring\n",
      "p sounds like fun ------------------- #tweetup sounds like fun\n",
      "Big booming thunder storm almost here.  Maybe we can all go home early???  Ah... probably not. ------------------- Big booming thunder storm almost here. Maybe we can all go home early??? Ah... probably not.\n",
      "great ------------------- twn..great\n",
      "excited ------------------- excited\n",
      "good morning ------------------- good morning\n",
      "its the best show EVER! ------------------- its the best show EVER!\n",
      "messed ------------------- messed\n",
      "i think iv hurt my tooth  and eilish and cassie are having a drawing competiton to draw cookies and pineapples haha :L . ------------------- i think iv hurt my tooth and eilish and cassie are having a drawing competiton to draw cookies and pineapples haha :L .\n",
      "I want to know when the auditions are Mander! Text or...reply please! ------------------- I want to know when the auditions are Mander! Text or...reply please!\n",
      "NOT THE SECRET NAMEREBECCA PLEASE ------------------- NOT THE SECRET NAMEREBECCA PLEASE\n",
      "miss ------------------- miss\n",
      "i need to get my computer fixed ------------------- i need to get my computer fixed\n",
      "illness ------------------- illness\n",
      "All the cool people I want to find for following today are #English, and I guess the English don`t tweet. ------------------- All the cool people I want to find for following today are #English, and I guess the English don`t tweet.\n",
      "no sir...i woulda put honey...but i don`t have any ------------------- no sir...i woulda put honey...but i don`t have any\n",
      "i totally loved it! ------------------- i totally loved it!\n",
      "i <3 you. u helped me thru the hrdest time of my life! ------------------- i <3 you. u helped me thru the hrdest time of my life!\n",
      "sad ------------------- sad\n",
      "Finally got a call for marriage counseling 3 days late.... ------------------- Finally got a call for marriage counseling 3 days late....\n",
      "ok then ------------------- ok then\n",
      "why baby? ------------------- why baby?\n",
      "sick! ------------------- sick!\n",
      "We`re having an impromptu pool party... Except I don`t know how to swim so I can`t get in ------------------- We`re having an impromptu pool party... Except I don`t know how to swim so I can`t get in\n",
      "oww ------------------- gum...oww\n",
      "happy 1 year! <3 ------------------- happy 1 year! <3\n",
      "Goooooooooooood Morrrrrrrrning ------------------- Goooooooooooood Morrrrrrrrning\n",
      "*phew*  Will make a note in case anyone else runs into the same issueï¿½ ------------------- *phew* Will make a note in case anyone else runs into the same issueï¿½\n",
      "WHAT ABOUT ME ??  I VOTE EVERY DAY FOR YOU !!!!! ------------------- WHAT ABOUT ME ?? I VOTE EVERY DAY FOR YOU !!!!!\n",
      "This diet is killing me ------------------- This diet is killing me\n",
      "i talk to you ------------------- i talk to you\n",
      "bored.. ------------------- bored...im\n",
      "e fun ------------------- have fun\n",
      "So far, so good, but I did sleep for most of those 4 hours. Getting a bit twitchy now ------------------- So far, so good, but I did sleep for most of those 4 hours. Getting a bit twitchy now\n",
      "What`s with Twatter lately?  Either I can`t get on or the replies don`t turn up! ------------------- What`s with Twatter lately? Either I can`t get on or the replies don`t turn up!\n",
      "lost my voice ------------------- lost my voice\n",
      "hate ------------------- hate\n",
      "I`ve heard this fall. I`m waiting too! ------------------- I`ve heard this fall. I`m waiting too!\n",
      "more nightmares?  *huggles* ------------------- more nightmares? *huggles*\n",
      "I AM SUCH A CREEPER  I feel disappointed because of it. **** my cyberstalking skills   the internet = no more privacy ------------------- I AM SUCH A CREEPER I feel disappointed because of it. **** my cyberstalking skills the internet = no more privacy.\n",
      "headache ------------------- headache\n",
      "happy ------------------- happy\n",
      "Grabbing coffee from  then making mom breakfast ------------------- Grabbing coffee from then making mom breakfast\n",
      "going to have fun ------------------- going to have fun\n",
      "thanks. before the major chop. ------------------- thanks. before the major chop.\n",
      "d thank you! ------------------- and thank you!\n",
      "just got up and updated my ipod ------------------- just got up and updated my ipod\n",
      "HAHAHA ------------------- HAHAHA\n",
      "crush ------------------- crush\n",
      "Saw James carville in the store today. His head is really that bald ------------------- Saw James carville in the store today. His head is really that bald\n",
      "yellow for   ? http://blip.fm/~5z05g ------------------- yellow for ? http://blip.fm/~5z05g\n",
      "which means you`re just going to have to come back to vancouver and have it our way! hahah ------------------- which means you`re just going to have to come back to vancouver and have it our way! hahah\n",
      "Feeling smooth ------------------- Feeling smooth\n",
      "Ew traffic ------------------- Ew traffic\n",
      "downloading songs while trying to sneak a lil homework in too, which should be my main priority not songs lol ------------------- downloading songs while trying to sneak a lil homework in too, which should be my main priority not songs lol\n",
      "your not alone...i need coffee too. ------------------- your not alone...i need coffee too.\n",
      "Sounds like me ------------------- Sounds like me\n",
      "Bad day  The day you realize what mess you`ve put me through will be one of the happiest days of my life... ------------------- Bad day The day you realize what mess you`ve put me through will be one of the happiest days of my life...\n",
      "I hate not having a bike.. ------------------- I hate not having a bike....especially\n",
      "_nesmith ------------------- _nesmith\n",
      "This is worse than taxes. ------------------- This is worse than taxes.\n",
      "JONAS BROTHERS - Live to party.                It`s rocking so hard ------------------- JONAS BROTHERS - Live to party. It`s rocking so hard\n",
      "happy ------------------- happy\n",
      "would love to test it though. ------------------- would love to test it though.\n",
      "Fun night! ------------------- Fun night!\n",
      "The exception for a short dude: Larenz fineass Tate yum ------------------- The exception for a short dude: Larenz fineass Tate yum\n",
      "I thought Wolverine was awesome. ------------------- I thought Wolverine was awesome.\n",
      "Hopefully ------------------- Hopefully\n",
      "Yes i work 6 to 3... ------------------- Yes i work 6 to 3...\n",
      "Hmmm, maybe that`s what they meant. They eluded to something brand new but you know how the media is ------------------- Hmmm, maybe that`s what they meant. They eluded to something brand new but you know how the media is\n",
      "Done at the spa   now meeting vic for some late lunch! ------------------- Done at the spa now meeting vic for some late lunch!\n",
      "Happy ------------------- Happy\n",
      "Always have wanted to go to Oz ------------------- Always have wanted to go to Oz\n",
      "Thx ------------------- Thx\n",
      "luv ------------------- luv\n",
      "that`s why I need to be there ------------------- that`s why I need to be there...To\n",
      "Okay so I`m dedicating my 300th tweet to the fact that I`m going to the Apple store because there is a HUGE crack on the glass screen! ------------------- Okay so I`m dedicating my 300th tweet to the fact that I`m going to the Apple store because there is a HUGE crack on the glass screen!\n",
      "If only we could ever actually be allowed to stay here and do that ------------------- If only we could ever actually be allowed to stay here and do that\n",
      "Let me know how that turns out!! ------------------- Let me know how that turns out!!\n",
      "was that sass I detect?  As long as it isn`t back sass! Haha ------------------- was that sass I detect? As long as it isn`t back sass! Haha\n",
      ",ahaha ------------------- ,ahaha\n",
      "Sports Bar Shatranjanpoli Rest Ph 26498457 All Sports Bar Andheri W 26733333 Dont know whether that helps. Google ki jai ho ------------------- Sports Bar Shatranjanpoli Rest Ph 26498457 All Sports Bar Andheri W 26733333 Dont know whether that helps. Google ki jai ho\n",
      "I`m not sleeping at all un ------------------- I`m not sleeping at all until\n",
      "Seriously, ------------------- Seriously,\n",
      "I live for pain, bring it on ------------------- I live for pain, bring it on\n",
      "okay, i`m out for a while  back later! ------------------- okay, i`m out for a while back later!\n",
      "g What is this powerblog challenge you keep talking about?  I`m a newbie followe ------------------- #powerblog What is this powerblog challenge you keep talking about? I`m a newbie follower\n",
      "died. ------------------- died.\n",
      "Waiting for tish to get off. Got to drive my moms CRV to pick her up all my myself and duckie. First time ------------------- Waiting for tish to get off. Got to drive my moms CRV to pick her up all my myself and duckie. First time\n",
      "not going to well! ------------------- not going to well!\n",
      "going to shower because i don`t want to smell at school tomorrow ------------------- going to shower because i don`t want to smell at school tomorrow\n",
      "Sigh... you know I am... ------------------- Sigh... you know I am...\n",
      "hopefully ------------------- hopefully\n",
      "Here are 4 FREE twitter tools will get you followers  http://short.to/511q http://jijr.com/hulz http://short.to/511r http://2ve.org/xPG0/ ------------------- Here are 4 FREE twitter tools will get you followers http://short.to/511q http://jijr.com/hulz http://short.to/511r http://2ve.org/xPG0/\n",
      "just got home from work.... and is chugging down a big bottle of apple juice. ------------------- just got home from work.... and is chugging down a big bottle of apple juice.\n",
      "g harmed ------------------- getting harmed\n",
      "what an amazing day. ------------------- what an amazing day.\n",
      "it was only once for my big brother...and I`m done now ------------------- it was only once for my big brother...and I`m done now\n",
      "good news: finally finished my #EASactive workout that has been paused for 6 hours. bad news: my resistance band is torn ------------------- good news: finally finished my #EASactive workout that has been paused for 6 hours. bad news: my resistance band is torn\n",
      "Well good morning all, What a wonderful day in the neighborhood  Thanks for all those that are now following another 60 this morning ------------------- Well good morning all, What a wonderful day in the neighborhood Thanks for all those that are now following another 60 this morning\n",
      "were amazing tonight.. ------------------- were amazing tonight..\n",
      "hope ------------------- hope\n",
      "it hurts too much... ------------------- it hurts too much...\n",
      "I waited, listening to wind blowing through the tumbleweed? Are none of you old enough to know what to do when someone says 'Crackerack'? ------------------- I waited, listening to wind blowing through the tumbleweed? Are none of you old enough to know what to do when someone says 'Crackerack'?\n",
      "Hell Yeah! ------------------- Hell Yeah!\n",
      "HIM shirt at dinner? Do you need to ask??  Does it actually have Ville on it? ------------------- HIM shirt at dinner? Do you need to ask?? Does it actually have Ville on it?\n",
      "i know!! ------------------- i know!!\n",
      "huh what the ****? Smelly? Noooo. I love alex Vixon ------------------- huh what the ****? Smelly? Noooo. I love alex Vixon\n",
      "GREAT ------------------- GREAT\n",
      "Happy b-day! ------------------- Happy b-day!\n",
      "thank ------------------- thank\n",
      "HAPPY ------------------- HAPPY\n",
      "glad ------------------- glad\n",
      "Thanks ------------------- Thanks\n",
      "twittering after 2 days! ------------------- twittering after 2 days!\n",
      "Its too nice ------------------- Its too nice\n",
      "depressed ------------------- depressed\n",
      "sick. ------------------- sick.\n",
      "Happy Mothers Day people. i love my mom a lot still ------------------- Happy Mothers Day people. i love my mom a lot still\n",
      "sorry ------------------- sorry\n",
      ", he was soooo friendly. ------------------- ago, he was soooo friendly.\n",
      "happy ------------------- happy\n",
      "Is getting the hang of Twitter. ------------------- Is getting the hang of Twitter.\n",
      "You know your neck is jacked up when you are forced to pay for parking bc you can`t turn you head to parallel park in the free spaces... ------------------- You know your neck is jacked up when you are forced to pay for parking bc you can`t turn you head to parallel park in the free spaces...\n",
      "I want it BACK NOW!: ------------------- I want it BACK NOW!:\n",
      "miss ------------------- miss\n",
      "bathing with two little angels, Keyla and Janice. ------------------- bathing with two little angels, Keyla and Janice.\n",
      "chocked ------------------- chocked\n",
      "HAHAHHA lol true that! i always remember my BD but i can never remember what date or even day it is ------------------- HAHAHHA lol true that! i always remember my BD but i can never remember what date or even day it is\n",
      "Family is here,hanging with them ------------------- Family is here,hanging with them\n",
      ".it`s gorgeous out! ------------------- ...it`s gorgeous out!\n",
      "m worried ------------------- I`m worried\n",
      "Im not bannished. ------------------- Im not bannished...\n",
      "Morning! If I get to see it, I`ll let you know. Right now, I`m going to go see Wolverine. ------------------- Morning! If I get to see it, I`ll let you know. Right now, I`m going to go see Wolverine.\n",
      "red top tabloids, build em up, knock em down ------------------- red top tabloids, build em up, knock em down\n",
      "tonight in party w/ my girls (minus vita) ------------------- tonight in party w/ my girls (minus vita)\n",
      "why not now you made me sad I thought you`d be jumping for joy ------------------- why not now you made me sad I thought you`d be jumping for joy\n",
      "Simple my a#@ ------------------- Simple my a#@\n",
      "The pics I just uploaded are the baby pics of my cats. Missy is now an adult and a pretty little kitty, but Batty is in kitten heaven now ------------------- The pics I just uploaded are the baby pics of my cats. Missy is now an adult and a pretty little kitty, but Batty is in kitten heaven now\n",
      "amazing ------------------- amazing\n",
      "glad ------------------- glad\n",
      "I have missed them ------------------- I have missed them\n",
      "_Live That`s what I want. More the better. Bound to be a few bad eggs though, but they will soon learn. ------------------- _Live That`s what I want. More the better. Bound to be a few bad eggs though, but they will soon learn.\n",
      "Hell yeah Kellynn got a Twitter. Finally. ------------------- Hell yeah Kellynn got a Twitter. Finally.\n",
      "as wort ------------------- was worth\n",
      "better ------------------- better\n",
      "Ship. I`m stuck. ------------------- Ship. I`m stuck.\n",
      "Cannot wait ------------------- Cannot wait\n",
      "shame ------------------- shame\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-103-73fa6e83cc0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m           \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"selected_text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m           \u001b[0;34m\"-------------------\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m           get_prediction_v3(train[\"text\"].iloc[i], train[\"selected_text\"].iloc[i]))\n\u001b[0m",
      "\u001b[0;32m<ipython-input-102-1d7a608325ee>\u001b[0m in \u001b[0;36mget_prediction_v3\u001b[0;34m(tweet, selected_text)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_prediction_v3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselected_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtweet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mselected_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselected_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    print(\n",
    "          train[\"selected_text\"].iloc[i], \\\n",
    "          \"-------------------\", \\\n",
    "          get_prediction_v3(train[\"text\"].iloc[i], train[\"selected_text\"].iloc[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 13, 18, 1, 3085, 15, 749, 20, 3798, 2676, 18, 28, 95, 1960, 1637, 18, 6872, 2483, 3]\n",
      "*sigh* k *sigh* k*sigh* speak *s\n"
     ]
    }
   ],
   "source": [
    "tweet = \"S`ok, trying to plot alternatives as we speak *sigh*\"\n",
    "selected_text = \"*sigh*\"\n",
    "\n",
    "\n",
    "selected_text_copy = \"\".join(selected_text.split())\n",
    "tweet_copy = \"\".join(tweet.split())\n",
    "\n",
    "len_st = len(selected_text_copy)\n",
    "idx0 = None\n",
    "idx1 = None\n",
    "\n",
    "for ind in (i for i, e in enumerate(tweet_copy) if e == selected_text_copy[0]):\n",
    "    if tweet_copy[ind: ind + len_st] == selected_text_copy:\n",
    "        idx0 = ind\n",
    "        idx1 = ind + len_st - 1\n",
    "        break\n",
    "        \n",
    "char_targets = [0] * len(tweet_copy)\n",
    "if idx0 != None and idx1 != None:\n",
    "    for ct in range(idx0, idx1 + 1):\n",
    "        char_targets[ct] = 1\n",
    "        \n",
    "input_ids_orig = tokenizer.encode(tweet)\n",
    "print(input_ids_orig)\n",
    "input_ids_orig = input_ids_orig[1:-1]\n",
    "\n",
    "tweet_offsets = []\n",
    "idx = 0\n",
    "for t in input_ids_orig:\n",
    "    w = tokenizer.decode([t])\n",
    "#     print(w, (idx, idx + len(w)))\n",
    "    tweet_offsets.append((idx, idx + len(w)))\n",
    "    idx += len(w)\n",
    "    \n",
    "target_idx = []\n",
    "for j, (offset1, offset2) in enumerate(tweet_offsets):\n",
    "    if sum(char_targets[offset1: offset2]) > 0:\n",
    "        target_idx.append(j)\n",
    "        \n",
    "# print(tokenizer.decode(13))\n",
    "\n",
    "targets_start = target_idx[0]\n",
    "targets_end = target_idx[-1]\n",
    "\n",
    "# ... i kinda lost respect  . i kinda lost respect\n",
    "prediction = \"\"\n",
    "for ix in range(targets_start, targets_end + 1):\n",
    "    prediction += tweet_copy[tweet_offsets[ix][0]: tweet_offsets[ix][1]]\n",
    "    if (ix + 1) < len(tweet_offsets) and tweet_offsets[ix][1] < tweet_offsets[ix + 1][0]:\n",
    "        prediction += \" \"\n",
    "        \n",
    "len_prediction = len(prediction)\n",
    "\n",
    "for ind in (i for i, e in enumerate(tweet) if e == prediction[0]):\n",
    "    \n",
    "    tweet_sub_sentence = \"\".join(tweet[ind:].split())\n",
    "    \n",
    "    if tweet_sub_sentence[:len_prediction] == prediction:\n",
    "        orig_idx0 = ind\n",
    "        break\n",
    "        \n",
    "for ind in range(orig_idx0, len(tweet)):\n",
    "\n",
    "    if \"\".join(tweet[orig_idx0: ind+1].split()) == prediction:\n",
    "        filtered_output = tweet[orig_idx0: ind+1]\n",
    "        break\n",
    "\n",
    "print(selected_text, filtered_output, prediction, tokenizer.decode(input_ids_orig[targets_start: targets_end+1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁',\n",
       " 's',\n",
       " '`',\n",
       " 'ok',\n",
       " ',',\n",
       " '▁trying',\n",
       " '▁to',\n",
       " '▁plot',\n",
       " '▁alternative',\n",
       " 's',\n",
       " '▁as',\n",
       " '▁we',\n",
       " '▁speak',\n",
       " '▁*',\n",
       " 's',\n",
       " 'igh',\n",
       " '*']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13,\n",
       " 18,\n",
       " 1,\n",
       " 3085,\n",
       " 15,\n",
       " 749,\n",
       " 20,\n",
       " 3798,\n",
       " 2676,\n",
       " 18,\n",
       " 28,\n",
       " 95,\n",
       " 1960,\n",
       " 1637,\n",
       " 18,\n",
       " 6872,\n",
       " 2483]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(tweet))\n",
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s<unk>ok, trying to plot alternatives as we speak *sigh*'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(encoded))\n",
    "decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 17\n"
     ]
    }
   ],
   "source": [
    "print(len(encoded), len(tokenizer.tokenize(tweet)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['S`ok,',\n",
       " 'trying',\n",
       " 'to',\n",
       " 'plot',\n",
       " 'alternatives',\n",
       " 'as',\n",
       " 'we',\n",
       " 'speak',\n",
       " '*sigh*']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁', 's', '`', 'ok', ',']\n",
      "['▁trying']\n",
      "['▁to']\n",
      "['▁plot']\n",
      "['▁alternative', 's']\n",
      "['▁as']\n",
      "['▁we']\n",
      "['▁speak']\n",
      "['▁*', 's', 'igh', '*']\n"
     ]
    }
   ],
   "source": [
    "for (i, word) in enumerate(tweet.split()):\n",
    "        \n",
    "    sub_tokens = tokenizer.tokenize(word)\n",
    "\n",
    "    print(sub_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3env",
   "language": "python",
   "name": "py3env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
